{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"examples/ai_functions/","title":"AI Functions Examples","text":""},{"location":"examples/ai_functions/#generate-a-list-of-fruits","title":"Generate a list of fruits","text":"<pre><code>from marvin import ai_fn\n@ai_fn\ndef list_fruits(n: int) -&gt; list[str]:\n\"\"\"Generate a list of n fruits\"\"\"\nlist_fruits(3) # [\"apple\", \"banana\", \"orange\"]\n</code></pre>"},{"location":"examples/ai_functions/#generate-fake-data-according-to-a-schema","title":"Generate fake data according to a schema","text":"<pre><code>from marvin import ai_fn\n@ai_fn\ndef fake_people(n: int) -&gt; list[dict]:\n\"\"\"\n    Generates n examples of fake data representing people, \n    each with a name and an age.\n    \"\"\"\nfake_people(3)\n# [{'name': 'John Doe', 'age': 29},\n#  {'name': 'Jane Smith', 'age': 34},\n#  {'name': 'Alice Johnson', 'age': 42}]\n</code></pre>"},{"location":"examples/ai_functions/#correct-spelling-and-grammar","title":"Correct spelling and grammar","text":"<pre><code>from marvin import ai_fn\n@ai_fn\ndef fix_sentence(sentence: str) -&gt; str:\n\"\"\"\n    Fix all grammatical and spelling errors in a sentence\n    \"\"\"\nfix_sentence(\"he go to mcdonald and buy burg\") # \"He goes to McDonald's and buys a burger.\"\n</code></pre>"},{"location":"examples/ai_functions/#cleaning-data","title":"Cleaning data","text":"<p>Cleaning data is such an important use case that Marvin has an entire module dedicated to it, including AI functions for categorization, standardization, entity extraction, and context-aware fills for missing values. See the data cleaning documentation for more information.</p>"},{"location":"examples/ai_functions/#unit-testing-llms","title":"Unit testing LLMs","text":"<p>One of the difficulties of building an AI library is unit testing it! While it's possible to make LLM outputs deterministic by setting the temperature to zero, a small change to a prompt could result in very different outputs. Therefore, we want a way to assert that an LLM's output is \"approximately equal\" to an expected value.</p> <p>This example is actually used by Marvin itself! See <code>marvin.utilities.tests.assert_llm()</code>.</p> <pre><code>@ai_fn()\ndef assert_llm(output: Any, expectation: Any) -&gt; bool:\n\"\"\"\n    Given the `output` of an LLM and an expectation, determines whether the\n    output satisfies the expectation.\n    For example:\n        `assert_llm(5, \"output == 5\")` will return `True` \n        `assert_llm(5, 4)` will return `False` \n        `assert_llm([\"red\", \"orange\"], \"a list of colors\")` will return `True` \n        `assert_llm([\"red\", \"house\"], \"a list of colors\")` will return `False`\n    \"\"\"\nassert_llm('Hello, how are you?', expectation='Hi there') # True\n</code></pre>"},{"location":"examples/ai_functions/#summarize-text","title":"Summarize text","text":"<p>This function takes any text and summarizes it. See the next example for a function that can also access Wikipedia automatically.</p> <pre><code>from marvin import ai_fn\n@ai_fn\ndef summarize(text: str) -&gt; str:\n\"\"\"\n    Summarize the provided text\n    \"\"\"\nimport wikipedia\npage = wikipedia.page('large language model')\nsummarize(text=page.content)\n# Large language models (LLMs) are neural networks with billions of parameters\n# trained on massive amounts of unlabelled text. They excel at various tasks and\n# can capture much of human language's syntax and semantics. LLMs use the\n# transformer architecture and are trained using unsupervised learning. Their\n# applications include fine-tuning and prompting for specific natural language\n# processing tasks.\n</code></pre>"},{"location":"examples/ai_functions/#summarize-text-after-loading-a-wikipedia-page","title":"Summarize text after loading a Wikipedia page","text":"<p>This example demonstrates how <code>ai_fn</code> can call a function to get additional information that can be used in producing a result. Here, the function downloads content from Wikipedia given a title.</p> <pre><code>from marvin import ai_fn\n@ai_fn\ndef summarize_from_wikipedia(title: str) -&gt; str:\n\"\"\"\n    Loads the wikipedia page corresponding to the provided \n    title and returns a summary of the content.\n    \"\"\"\nimport wikipedia\npage = wikipedia.page(title)\n# the content to summarize\nyield page.content\nsummarize_from_wikipedia(title='large language model')\n# A large language model (LLM) is a language model consisting of a neural\n# network with many parameters (typically billions of weights or more), trained\n# on large quantities of unlabelled text using self-supervised learning. LLMs\n# emerged around 2018 and perform well at a wide variety of tasks. This has\n# shifted the focus of natural language processing research away from the\n# previous paradigm of training specialized supervised models for specific\n# tasks.\n</code></pre>"},{"location":"examples/ai_functions/#suggest-a-title-after-loading-a-url","title":"Suggest a title after loading a URL","text":"<p>This example demonstrates how <code>ai_fn</code> can call a function to get additional information that can be used in producing a result. Here, the function loads an article and then suggests a title for it.</p> <pre><code>from marvin import ai_fn\n@ai_fn\ndef suggest_title(url: str) -&gt; str:\n\"\"\"\n    Suggests a title for the article found at the provided URL\n    \"\"\"\nimport httpx\n# load the url\nresponse = httpx.get(url)\n# return the url contents \nyield marvin.utilities.strings.html_to_content(response.content)\nsuggest_title(url=\"https://techcrunch.com/2023/03/14/openai-releases-gpt-4-ai-that-it-claims-is-state-of-the-art/\")\n# OpenAI Releases GPT-4: State-of-the-Art AI Model with Improved Image and Text Understanding\n</code></pre>"},{"location":"examples/ai_functions/#generate-rhymes","title":"Generate rhymes","text":"<pre><code>from marvin import ai_fn\n@ai_fn\ndef rhyme(word: str) -&gt; str:\n\"\"\"\n    Generate a word that rhymes with the supplied `word`\n    \"\"\"\nrhyme(\"blue\") # glue\n</code></pre>"},{"location":"examples/ai_functions/#find-words-meeting-specific-criteria","title":"Find words meeting specific criteria","text":"<pre><code>from marvin import ai_fn\n@ai_fn\ndef find_words(text: str, criteria: str) -&gt; list[str]:\n\"\"\"\n    Given text and some criteria, returns a list of \n    every word meeting that criteria.\n    \"\"\"\ntext = \"The quick brown fox jumps over the lazy dog.\"\nfind_words(text, criteria=\"adjectives\") # [\"quick\", \"brown\", \"lazy\"]\nfind_words(text, criteria=\"colors\") # [\"brown\"]\nfind_words(text, criteria=\"animals that aren't dogs\") # [\"fox\"]\n</code></pre>"},{"location":"examples/ai_functions/#suggest-emojis","title":"Suggest emojis","text":"<pre><code>from marvin import ai_fn\n@ai_fn\ndef get_emoji(text: str) -&gt; str:\n\"\"\"\n    Returns an emoji that describes the provided text.\n    \"\"\"\nget_emoji(\"incredible snack\") # '\ud83c\udf7f'\n</code></pre>"},{"location":"examples/ai_functions/#generate-rrules","title":"Generate RRULEs","text":"<p>RRULE strings are standardized representations of calendar events. This AI function can convert natural language into an RRULE.</p> <p>This is also available as a builtin function: <code>marvin.ai_functions.strings.rrule</code></p> <pre><code>from marvin import ai_fn\n@ai_fn\ndef rrule(text: str) -&gt; str:\n\"\"\"\n    Generate valid RRULE strings from a natural language description of an event\n    \"\"\"\nyield pendulum.now.isoformat()\nrrule('every hour from 9-6 on thursdays')\n# \"RRULE:FREQ=WEEKLY;BYDAY=TH;BYHOUR=9,10,11,12,13,14,15,16;BYMINUTE=0;BYSECOND=0\"\n</code></pre>"},{"location":"examples/ai_functions/#get-a-datetime-from-a-natural-language-description","title":"Get a datetime from a natural language description","text":"<pre><code>from datetime import datetime\nfrom marvin import ai_fn\n@ai_fn\ndef make_datetime(description: str, tz: str = \"BST\") -&gt; datetime:\n\"\"\" generates a datetime from a description \"\"\"\n# !date +\"%Y-%m-%d %T %Z\"\n# 2023-06-23 22:30:25 BST\ndt = make_datetime(\"5 mins from now\")\n# datetime.datetime(2023, 6, 23, 22, 35, tzinfo=datetime.timezone(datetime.timedelta(seconds=3600)))\ndt.isoformat()\n# '2023-06-23T22:35:00+01:00'\n</code></pre>"},{"location":"examples/ai_models/","title":"AI Models Examples","text":""},{"location":"examples/ai_models/#structure-conversational-user-input","title":"Structure conversational user input","text":"<pre><code>from marvin import ai_model\nfrom typing import Optional, List\nimport datetime\nimport pydantic\nclass Destination(pydantic.BaseModel):\nstart: datetime.date\nend: datetime.date\ncity: Optional[str]\ncountry: str\nsuggested_attractions: list[str]\n@ai_model\nclass Trip(pydantic.BaseModel):\ntrip_start: datetime.date\ntrip_end: datetime.date\ntrip_preferences: list[str]\ndestinations: List[Destination]\nTrip('''\\\nI've got all of June off, so hoping to spend the first\\\nhalf of June in London and the second half in Rabat. I love \\\ngood food and going to museums.\n''')\n# {\n#   \"trip_start\": \"2023-06-01\",\n#   \"trip_end\": \"2023-06-30\",\n#   \"trip_preferences\": [\n#     \"good food\",\n#     \"museums\"\n#   ],\n#   \"destinations\": [\n#     {\n#       \"start\": \"2023-06-01\",\n#       \"end\": \"2023-06-15\",\n#       \"city\": \"London\",\n#       \"country\": \"United Kingdom\",\n#       \"suggested_attractions\": [\n#         \"British Museum\",\n#         \"Tower of London\",\n#         \"Borough Market\"\n#       ]\n#     },\n#     {\n#       \"start\": \"2023-06-16\",\n#       \"end\": \"2023-06-30\",\n#       \"city\": \"Rabat\",\n#       \"country\": \"Morocco\",\n#       \"suggested_attractions\": [\n#         \"Kasbah des Oudaias\",\n#         \"Hassan Tower\",\n#         \"Rabat Archaeological Museum\"\n#       ]\n#     }\n#   ]\n# }\n</code></pre>"},{"location":"examples/ai_models/#format-electronic-health-records-data-declaratively","title":"Format electronic health records data declaratively","text":"<pre><code>from datetime import date\nfrom typing import Optional, List\nfrom pydantic import BaseModel\nclass Patient(BaseModel):\nname: str\nage: int\nis_smoker: bool\nclass Diagnosis(BaseModel):\ncondition: str\ndiagnosis_date: date\nstage: Optional[str] = None\ntype: Optional[str] = None\nhistology: Optional[str] = None\ncomplications: Optional[str] = None\nclass Treatment(BaseModel):\nname: str\nstart_date: date\nend_date: Optional[date] = None\nclass Medication(Treatment):\ndose: Optional[str] = None\nclass BloodTest(BaseModel):\nname: str\nresult: str\ntest_date: date\n@ai_model\nclass PatientData(BaseModel):\npatient: Patient\ndiagnoses: List[Diagnosis]\ntreatments: List[Treatment]\nblood_tests: List[BloodTest]\nPatientData('''\\\nMs. Lee, a 45-year-old patient, was diagnosed with type 2 diabetes mellitus on 06-01-2018.\nUnfortunately, Ms. Lee's diabetes has progressed and she developed diabetic retinopathy on 09-01-2019.\nMs. Lee was diagnosed with type 2 diabetes mellitus on 06-01-2018.\nMs. Lee was initially diagnosed with stage I hypertension on 06-01-2018.\nMs. Lee's blood work revealed hyperlipidemia with elevated LDL levels on 06-01-2018.\nMs. Lee was prescribed metformin 1000 mg daily for her diabetes on 06-01-2018.\nMs. Lee's most recent A1C level was 8.5% on 06-15-2020.\nMs. Lee was diagnosed with type 2 diabetes mellitus, with microvascular complications, including diabetic retinopathy, on 09-01-2019.\nMs. Lee's blood pressure remains elevated and she was prescribed lisinopril 10 mg daily on 09-01-2019.\nMs. Lee's most recent lipid panel showed elevated LDL levels, and she was prescribed atorvastatin 40 mg daily on 09-01-2019.\\\n''')\n# {\n#   \"patient\": {\n#     \"name\": \"Ms. Lee\",\n#     \"age\": 45,\n#     \"is_smoker\": false\n#   },\n#   \"diagnoses\": [\n#     {\n#       \"condition\": \"Type 2 diabetes mellitus\",\n#       \"diagnosis_date\": \"2018-06-01\",\n#       \"stage\": \"I\",\n#       \"type\": null,\n#       \"histology\": null,\n#       \"complications\": null\n#     },\n#     {\n#       \"condition\": \"Diabetic retinopathy\",\n#       \"diagnosis_date\": \"2019-09-01\",\n#       \"stage\": null,\n#       \"type\": null,\n#       \"histology\": null,\n#       \"complications\": null\n#     }\n#   ],\n#   \"treatments\": [\n#     {\n#       \"name\": \"Metformin\",\n#       \"start_date\": \"2018-06-01\",\n#       \"end_date\": null\n#     },\n#     {\n#       \"name\": \"Lisinopril\",\n#       \"start_date\": \"2019-09-01\",\n#       \"end_date\": null\n#     },\n#     {\n#       \"name\": \"Atorvastatin\",\n#       \"start_date\": \"2019-09-01\",\n#       \"end_date\": null\n#     }\n#   ],\n#   \"blood_tests\": [\n#     {\n#       \"name\": \"A1C\",\n#       \"result\": \"8.5%\",\n#       \"test_date\": \"2020-06-15\"\n#     },\n#     {\n#       \"name\": \"LDL\",\n#       \"result\": \"Elevated\",\n#       \"test_date\": \"2018-06-01\"\n#     },\n#     {\n#       \"name\": \"LDL\",\n#       \"result\": \"Elevated\",\n#       \"test_date\": \"2019-09-01\"\n#     }\n#   ]\n# }\n</code></pre>"},{"location":"examples/ai_models/#text-to-orm","title":"Text to ORM","text":"<pre><code>from datetime import date\nfrom typing import Optional\nfrom pydantic import BaseModel\nfrom django.db.models import Q\nclass DjangoLookup(BaseModel):\nfield: Literal[*django_fields]\nlookup: Literal[*django_lookups] = pydantic.Field(description = 'e.g. __iregex')\nvalue: Any\n@ai_model\nclass DjangoQuery(BaseModel):\n''' A model represneting a Django ORM query'''\nlookups: List[DjangoLookup]\ndef to_q(self) -&gt; Q:\nq = Q()\nfor lookup in self.lookups:\nq &amp;= Q(**{f\"{lookup.field}__{lookup.lookup}\": lookup.value})\nreturn q\nDjangoQuery('''\\\n    All users who joined more than two months ago but\\\n    haven't made a purchase in the last 30 days'''\n).to_q()\n# &lt;Q: (AND: \n#     ('date_joined__lte', '2023-03-11'), \n#     ('last_purchase_date__isnull', False), \n#     ('last_purchase_date__lte', '2023-04-11'))&gt;\n</code></pre>"},{"location":"examples/ai_models/#extract-financial-information-from-messy-csv-data","title":"Extract financial information from messy CSV data","text":"<pre><code>from datetime import date\nfrom typing import Optional\nfrom pydantic import BaseModel\n@ai_model\nclass CapTable(pydantic.BaseModel):\ntotal_authorized_shares: int\ntotal_common_share: int\ntotal_common_shares_outstanding: Optional[int]\ntotal_preferred_shares: int\nconversion_price_multiple: int = 1\nCapTable('''\\\nIn the cap table for Charter, the total authorized shares amount to 13,250,000. \nThe total number of common shares stands at 10,000,000 as specified in Article Fourth, \nclause (i) and Section 2.2(a)(i). The exact count of common shares outstanding is not \navailable at the moment. Furthermore, there are a total of 3,250,000 preferred shares mentioned \nin Article Fourth, clause (ii) and Section 2.2(a)(ii). The dividend percentage for Charter is \nset at 8.00%. Additionally, the mandatory conversion price multiple is 3x, which is \nderived from the Term Sheet.\\\n''')\n# {\n#   \"total_authorized_shares\": 13250000,\n#   \"total_common_share\": 10000000,\n#   \"total_common_shares_outstanding\": null,\n#   \"total_preferred_shares\": 3250000,\n#   \"conversion_price_multiple\": 3\n# }\n</code></pre>"},{"location":"examples/ai_models/#extract-action-items-from-meeting-transcripts","title":"Extract action items from meeting transcripts","text":"<pre><code>from marvin import ai_model\nimport datetime\nfrom typing import Literal, List\nimport pydantic\nclass ActionItem(pydantic.BaseModel):\nresponsible: str\ndescription: str\ndeadline: Optional[datetime.datetime]\ntime_sensitivity: Literal['low', 'medium', 'high']\n@ai_model\nclass Conversation(pydantic.BaseModel):\n'''A class representing a team converastion'''\nparticipants: List[str]\naction_items: List[ActionItem]\nConversation('''\nAdam: Hey Jeremiah can you approve my PR? I requested you to review it.\nJeremiah: Yeah sure, when do you need it done by?\nAdam: By this Friday at the latest, we need to ship it by end of week.\nJeremiah: Oh shoot, I need to make sure that Nate and I have a chance to chat first.\nNate: Jeremiah we can meet today to chat.\nJeremiah: Okay, I'll book something for today.\n''')\n# {\n#   \"participants\": [\n#     \"Adam\",\n#     \"Jeremiah\",\n#     \"Nate\"\n#   ],\n#   \"action_items\": [\n#     {\n#       \"responsible\": \"Jeremiah\",\n#       \"description\": \"Approve Adam's PR\",\n#       \"deadline\": \"2023-05-12T23:59:59\",\n#       \"time_sensitivity\": \"high\"\n#     },\n#     {\n#       \"responsible\": \"Jeremiah\",\n#       \"description\": \"Book a meeting with Nate\",\n#       \"deadline\": \"2023-05-11T23:59:59\",\n#       \"time_sensitivity\": \"high\"\n#     }\n#   ]\n# }\n</code></pre>"},{"location":"examples/ai_models/#schema-normalization-for-data-warehousing","title":"Schema normalization for data warehousing","text":"<pre><code>from marvin import ai_model\nimport datetime\nfrom typing import Literal, List\nimport pydantic\nclass YourSchema(pydantic.BaseModel):\n'''A profile representing a user'''\nfirst_name: str\nlast_name: str\nphone_number: str\nemail: str\ndate_joined: datetime.datetime\n@ai_model\nclass MySchema(pydantic.BaseModel):\n'''A profile representing a user'''\ngiven_name: str\nfamily_name: str\ncontact_number: str\nemail_address: str\ndatetime_created: datetime.datetime\n# I want the data in my schema.\nMySchema(\n# But I only have data from your schema.\nYourSchema(\nfirst_name = 'Ford',\nlast_name = 'Prefect',\nphone_number = '555-555-5555',\nemail = 'ford@prefect.io',\ndate_joined ='2022-05-11T23:59:59'\n).json()\n)\n# {\n#   \"given_name\": \"Ford\",\n#   \"family_name\": \"Prefect\",\n#   \"contact_number\": \"555-555-5555\",\n#   \"email_address\": \"ford@prefect.io\",\n#   \"datetime_created\": \"2022-05-11T23:59:59\"\n# }\n</code></pre>"},{"location":"examples/ai_models/#structure-data-from-scraping-web-pages","title":"Structure data from scraping web pages","text":"<pre><code>from marvin import ai_model\nimport pydantic\nimport requests\nfrom bs4 import BeautifulSoup as soup\n@ai_model\nclass Company(pydantic.BaseModel):\nname: str\nindustries: List[str]\ndescription_short: str\ndescription_long: str\nproducts: List[str]\nresponse = requests.get('https://www.apple.com')\ntext = soup(response.content).get_text(separator = ' ', strip = True)\nCompany(text)\n# {\n#   \"name\": \"Apple\",\n#   \"industries\": [\n#     \"Technology\",\n#     \"Consumer electronics\"\n#   ],\n#   \"description_short\": \"Apple is a multinational technology company that designs, develops, and sells consumer electronics, computer software, and online services.\",\n#   \"description_long\": \"Apple Inc. is an American multinational technology company that designs, develops, and sells consumer electronics, computer software, and online services. The company's hardware products include the iPhone smartphone, the iPad tablet computer, the Mac personal computer, the iPod portable media player, the Apple Watch smartwatch, the Apple TV digital media player, and the HomePod smart speaker. Apple's software includes the macOS, iOS, iPadOS, watchOS, and tvOS operating systems, the iTunes media player, the Safari web browser, and the iLife and iWork creativity and productivity suites. The online services include the iTunes Store, the iOS App Store, and Mac App Store, Apple Music, and iCloud. The company was founded on April 1, 1976, and incorporated on January 3, 1977, by Steve Jobs, Steve Wozniak, and Ronald Wayne.\",\n#   \"products\": [\n#     \"iPhone\",\n#     \"iPad\",\n#     \"Mac\",\n#     \"iPod\",\n#     \"Apple Watch\",\n#     \"Apple TV\",\n#     \"HomePod\",\n#     \"macOS\",\n#     \"iOS\",\n#     \"iPadOS\",\n#     \"watchOS\",\n#     \"tvOS\",\n#     \"iTunes\",\n#     \"Safari\",\n#     \"iLife\",\n#     \"iWork\",\n#     \"iTunes Store\",\n#     \"iOS App Store\",\n#     \"Mac App Store\",\n#     \"Apple Music\",\n#     \"iCloud\"\n#   ]\n# }\n</code></pre>"},{"location":"examples/ai_models/#smart-routing-in-application-development","title":"Smart routing in application development","text":"<pre><code>from marvin import ai_model\nimport datetime\nfrom typing import Literal, List\nimport pydantic\n@ai_model\nclass Router(pydantic.BaseModel):\n'''A class representing an AI-based router'''\nrequest: str = pydantic.Field(description = 'Raw user query')\npage: Literal['/profile', '/billing', '/metrics', '/refunds']\nRouter('''I want to update my address''')\n# {\n#   \"request\": \"I want to update my address\",\n#   \"page\": \"/profile\"\n# }\n</code></pre>"},{"location":"guide/ai_components/ai_functions/","title":"AI Functions","text":"<p>Features</p> <p>\ud83c\udf89 Create AI functions with a single <code>@ai_fn</code> decorator</p> <p>\ud83e\udde0 Use an LLM as an execution runtime to generate outputs without code</p> <p>\ud83e\uddf1 Use native data structures as inputs and outputs</p> <p>Use Cases</p> <p>\ud83c\udfd7\ufe0f NLP pipelines</p> <p>\ud83e\uddea Synthetic data generation</p> <p>\u2705 Sentiment analysis</p> <p>AI functions are functions that are defined locally but use AI to generate their outputs. Like normal functions, AI functions take arguments and return structured outputs like <code>lists</code>, <code>dicts</code> or even Pydantic models. Unlike normal functions, they don't need any source code! </p> <p>Consider the following example, which contains a function that generates a list of fruits. The function is defined with a descriptive name, annotated input and return types, and a docstring -- but doesn't appear to actually do anything. Nonetheless, because of the <code>@ai_fn</code> decorator, it can be called like a normal function and returns a list of fruits.</p> <pre><code>from marvin import ai_fn\n@ai_fn\ndef list_fruits(n: int) -&gt; list[str]:\n\"\"\"Generate a list of n fruits\"\"\"\nlist_fruits(n=3) # [\"apple\", \"banana\", \"orange\"]\n</code></pre> <p>Tip</p> <p>AI functions work best with GPT-4, but results are still very good with GPT-3.5.</p> <p>... or another example, this time returning a finite set of <code>Literal</code> values to classify GitHub issues:</p> <pre><code>from typing_extensions import Literal\nfrom marvin import ai_fn\nIssueTag = Literal['bug', 'docs', 'enhancement', 'feature']\n@ai_fn\ndef issue_classifier(issue_body: str) -&gt; list[IssueTag]:\n\"\"\" returns appropriate issue tags given an issue body \"\"\"\nissue_classifier(\"\"\"\n    yeah so i tried using the tui and it teleported me to another dimension.\n    also there's a typo on the ai_fn's page, you forgot the code\n\"\"\")\n# ['bug', 'docs']\n</code></pre>"},{"location":"guide/ai_components/ai_functions/#when-to-use-ai-functions","title":"When to use AI functions","text":"<p>Because AI functions look and feel just like normal functions, they are the easiest way to add AI capabilities to your code -- just write the definition of the function you want to call, and use it anywhere! However, though they can feel like magic, it's important to understand that there are times you should prefer not to use AI functions.</p> <p>Modern LLMs are extremely powerful, especially when working with natural language and ideas that are easy to discuss but difficult to describe algorithmically. However, since they don't actually execute code, computing extremely precise results can be surprisingly difficult. Asking an AI to compute an arithmetic expression is a lot like asking a human to do the same -- it's possible they'll get the right answer, but you'll probably want to double check on a calculator. On the other hand, you wouldn't ask the calculator to rewrite a paragraph as a poem, which is a perfectly natural thing to ask an AI. Bear in mind that AI functions are (relatively) slow and expensive compared to running code on your computer. </p> <p>Therefore, while there are many appropriate times to use AI functions, it's important to note that they complement normal functions incredibly well and to know when to use one or the other. AIs tend to excel at exactly the things that are very hard to describe algorithmically. If you're doing matrix multiplication, use a normal function. If you're extracting all the animals that are native to Europe from text, use an AI function.</p> <p>Here is a guide for when to use AI functions:</p> <ul> <li>Generating data (any kind of text, but also data matching a certain structure or template)</li> <li>Translating or rewriting text</li> <li>Summarization</li> <li>Sentiment analysis</li> <li>Keyword or entity extraction</li> <li>Asking qualitative questions about quantitative data</li> <li>Fixing spelling or other errors</li> <li>Generating outlines or action items</li> <li>Transforming one data structure to another</li> </ul> <p>Here is a guide for when NOT to use AI functions:</p> <ul> <li>The function is easy to write normally</li> <li>You want to be able to debug the function</li> <li>You require deterministic outputs</li> <li>Precise math beyond basic arithmetic</li> <li>You need any type of side effect or IO (AI functions are not \"executed\" in a traditional sense, so they can't interact with your computer or network)</li> <li>The objective is TOO magic (tempting though it may be, you can't write an AI function to solve an impossible problem)</li> </ul>"},{"location":"guide/ai_components/ai_functions/#basic-usage","title":"Basic usage","text":"<p>The <code>ai_fn</code> decorator can be applied to any function. For best results, the function should have an informative name, annotated input types, a return type, and a docstring. The function does not need to have any source code written, but advanced users can add source code to influence the output in two different ways (see \"writing source code\")</p> <p>When a <code>ai_fn</code>-decorated function is called, all available information is sent to the AI, which generates a predicted output. This output is parsed and returned as the function result.</p> <pre><code>from marvin import ai_fn\n@ai_fn\ndef my_function(input: Type) -&gt; ReturnType:\n\"\"\" \n    A docstring that describes the function's purpose and behavior.\n    \"\"\"\n# call the function\nmy_function(input=\"my input\")\n</code></pre> <p>Note the following:</p> <ol> <li>Apply the decorator to the function. It does not need to be called (though it can take optional arguments)</li> <li>The function should have a descriptive name</li> <li>The function's inputs should have type annotations</li> <li>The function's return type should be annotated</li> <li>The function has a descriptive docstring</li> <li>The function does not need any source code!</li> </ol>"},{"location":"guide/ai_components/ai_functions/#advanced-usage","title":"Advanced usage","text":""},{"location":"guide/ai_components/ai_functions/#customizing-the-llm","title":"Customizing the LLM","text":"<p>By default, AI functions use Marvin's global LLM settings. However, you can change this on a per-function basis by providing a valid model name or temperature to the <code>@ai_fn</code> decorator.</p> <p>For example, this function will always use GPT-3.5 with a temperature of 0.2. <pre><code>from marvin import ai_fn\n@ai_fn(llm_model='gpt-3.5-turbo', llm_temperature=0.2)\ndef my_function():\n...\n</code></pre></p>"},{"location":"guide/ai_components/ai_functions/#deterministic-ai-functions","title":"Deterministic AI functions","text":"<p>LLM implementations like ChatGPT are non-deterministic; that is, they do not always return the same output for a given input. In some use cases, like natural conversation, this is desireable. In others, especially programmatic ones, it is not. You can control this by setting the model <code>temperature</code>. High temperature leads to greater variation in responses; a temperature of 0 will always give the same response for a given input. Marvin's default temperature is 0.8. To create a deterministic AI function, set its temperature to 0.</p> <p>Upgrading Marvin may change AI function outputs</p> <p>Marvin wraps your AI function with additional prompts in order to get the LLM to generate parseable outputs. We are constantly adjusting those prompts to improve performance and address edge cases. Therefore, even with a temperature of 0, what your AI function sends to the LLM might change if you upgrade from one version of Marvin to another, resulting in different AI function outputs. Therefore, AI functions with <code>temperature=0</code> are only guaranteed to be deterministic for a specific Marvin version. </p>"},{"location":"guide/ai_components/ai_functions/#getting-values-at-runtime","title":"Getting values at runtime","text":"<p>By default, the <code>ai_fn</code> decorator will not attempt to run your function, even if it has source code. The LLM will attempt to predict everything itself. However, sometimes it is useful to manipulate the function at runtime. There are two ways to do this.</p> <p>The first is to wrap your function in another function or decorator. For example, here we write a private AI function to extract keywords from text and call it from a user-facing function that accepts a URL and loads the actual content.</p> <pre><code>import httpx\nfrom marvin import ai_fn\n@ai_fn\ndef _get_keywords(text:str) -&gt; list[str]:\n\"\"\"extract keywords from string\"\"\"\ndef get_keywords_from_url(url: str) -&gt; list[str]:\nresponse = httpx.get(url)\nreturn _get_keywords(response.content)\n</code></pre> <p>An advanced alternative is to yield from the ai function itself. This is supported for a single yield statement:</p> <pre><code>import httpx\nfrom marvin import ai_fn\n@ai_fn\ndef get_keywords_from_url(url: str) -&gt; list[str]:\n\"\"\"Extract keywords from the text of a URL\"\"\"\nresponse = httpx.get(url)\nyield response.content\n</code></pre> <p>In this case, the function will be run up to the yield statement, and the yielded value will be considered by the LLM when generating a result. Therefore, these two examples are equivalent. It may be that the first example is more legible than the second, which is extremely magical.</p>"},{"location":"guide/ai_components/ai_functions/#async-functions","title":"Async functions","text":"<p>The <code>ai_fn</code> decorator works with async functions.</p> <pre><code>from marvin import ai_fn\n@ai_fn\nasync def f(x: int) -&gt; int:\n\"\"\"Add 100 to x\"\"\"\nawait f(5)\n</code></pre>"},{"location":"guide/ai_components/ai_functions/#complex-annotations","title":"Complex annotations","text":"<p>Annotations don't have to be types; they can be complex objects or even string descriptions. For inputs, the annotation is transmitted to the AI as-is. Return annotations are processed through Marvin's <code>ResponseFormatter</code> mechanism, which puts extra emphasis on compliance. This means you can supply complex instructions in your return annotation. However, note that you must include the word <code>json</code> in order for Marvin to automatically parse the result into native objects!</p> <p>Therefore, consider these two approaches to defining an output: <pre><code>from marvin import ai_fn\n@ai_fn\ndef fn_with_docstring(n: int) -&gt; list[dict]:\n\"\"\"\n    Generate a list of n people with names and ages\n    \"\"\"\n@ai_fn\ndef fn_with_string_annotation(n: int) -&gt; 'a json list of dicts that have keys for name and age':\n\"\"\"\n    Generate a list of n people\n    \"\"\"\nclass Person(pydantic.BaseModel):\nname: str\nage: int\n@ai_fn\ndef fn_with_structured_annotation(n: int) -&gt; list[Person]:\n\"\"\"\n    Generate a list of n people\n    \"\"\"\n</code></pre> All three of these functions will give similar output (though the last one, <code>fn_with_structured_annotation</code>, will return Pydantic models instead of dicts). However, they are increasingly specific in their instructions to the AI. While you should always try to make your intent as clear as possible to the AI, you should also choose an approach that will make sense to other people reading your code. This would lead us to probably prefer the first or third functions over the second, which doesn't look like a typical Python function.</p>"},{"location":"guide/ai_components/ai_functions/#plugins","title":"Plugins","text":"<p>AI functions are powered by bots, so they can also use plugins. Unlike bots, AI functions have no plugins available by default in order to minimize the possibility of confusing behavior. You can provide plugins when you create the AI function:</p> <pre><code>@ai_fn(plugins=[...])\ndef my_function():\n...\n</code></pre>"},{"location":"guide/ai_components/ai_models/","title":"AI Models","text":"<p>Features</p> <p>\ud83e\uddf1 Drop-in replacment for Pydantic models that can be instantiated from natural language</p> <p>\ud83d\udd17 Transform raw text into type-safe outputs</p> <p>\ud83c\udf89 Create with a single <code>@ai_model</code> decorator</p> <p>Use Cases</p> <p>\ud83c\udfd7\ufe0f Entity extraction</p> <p>\ud83e\uddea Synthetic data generation</p> <p>\u2705 Standardization</p> <p>\ud83e\udde9 Type-safety</p> <p>AI models are Pydantic models that are defined locally and use AI to process their inputs at runtime. Like normal Pydantic models, AI models define a schema that data must comply with. Unlike normal Pydantic models, AI models can handle unstructured text and automatically convert it into structured, type-safe outputs without requiring any additional source code!</p> <p>To create an AI model, decorate any Pydantic model with <code>@ai_model</code>.</p> <p><pre><code>import pydantic\nfrom marvin import ai_model\n@ai_model\nclass Resume(pydantic.BaseModel):\nfirst_name: str\nlast_name: str\nphone_number: str = pydantic.Field(None, description='dash-delimited phone number')\nemail: str\n</code></pre> You can now create structured <code>Resume</code> objects from raw text:  <pre><code>Resume('Ford Prefect \u2022 (555) 5124-5242 \u2022 ford@prefect.io')\n</code></pre> This produces a Pydantic model that is exactly equivalent to: <pre><code>Resume(\nfirst_name='Ford',\nlast_name='Prefect',\nphone_number='555-5124-5242',\nemail='ford@prefect.io'\n)\n</code></pre></p> <p>Tip</p> <p>AI models work best with GPT-4, but results are still very good with GPT-3.5.</p>"},{"location":"guide/ai_components/ai_models/#when-to-use-ai-models","title":"When to use AI Models","text":"<p>Because AI models integrate seamlessly with the Pydantic framework, they are a straightforward way to infuse AI capabilities into your data processing pipeline. Just define the Pydantic model with the fields you want to extract from the unstructured text and use it anywhere! However, even though they can feel like magic, it's crucial to understand that there are situations where you might prefer not to use AI models.</p> <p>Modern LLMs are extraordinarily potent, particularly when dealing with natural language and concepts that are simple to express but challenging to encode algorithmically. However, because they don't actually execute code, computing precise results can be tricky. </p> <p>For example, asking an AI to summarize intricate legal language is akin to asking a human to do the same -- it's feasible they'll comprehend the right context, but you'll probably want to double-check with a legal expert. On the other hand, you wouldn't ask the legal expert to summarize a complex research paper, which is a perfectly natural thing to ask an AI. </p> <p>Therefore, while there are many suitable times to use AI models, they aren't always a great fit. AI models tend to excel at exactly the things that are hard to codify algorithmically. If you're performing simple data validation, use a normal Pydantic model. If you're extracting context from unstructured text, use an AI model.</p>"},{"location":"guide/ai_components/ai_models/#basic-usage","title":"Basic usage","text":"<p>The <code>ai_model</code> decorator can be applied to any Pydantic model. For optimal results, the model should have a descriptive name, annotated fields, and a class docstring. The model does not need to have any pre-processing or post-processing methods written. However, advanced users can add these methods to influence the output.</p> <p>When an <code>ai_model</code>-decorated class is instantiated with unstructured text, all available information is sent to the AI. The AI generates a predicted output that is parsed according to the model's schema and returned as an instance of the model.</p> <p><pre><code>import pydantic\nfrom marvin import ai_model\n@ai_model\nclass Resume(pydantic.BaseModel):\nfirst_name: str\nlast_name: str\nphone_number: str = pydantic.Field(None, description='dash-delimited phone number')\nemail: str\nResume('Ford Prefect \u2022 (555) 5124-5242 \u2022 ford@prefect.io')\n</code></pre> This produces a Pydantic model that is exactly equivalent to: <pre><code>Resume(\nfirst_name='Ford',\nlast_name='Prefect',\nphone_number='555-5124-5242',\nemail='ford@prefect.io'\n)\n</code></pre></p>"},{"location":"guide/ai_components/ai_models/#advanced-usage","title":"Advanced usage","text":"<p>In addition to basic text instantiation, you can customize the behavior of AI models in a few ways. </p>"},{"location":"guide/ai_components/ai_models/#model-customization","title":"Model customization","text":"<p>To customize models for your entire application, use Marvin's global settings object. To customize the model for a specific AI Model, pass an appropriate Model configuration:</p> <pre><code>from marvin import ai_model\nfrom marvin.engine.language_models import chat_llm\n@ai_model(model=chat_llm(temperature=0.2, model='gpt-3.5-turbo'))\nclass MyModel(BaseModel):\n...\n</code></pre>"},{"location":"guide/ai_components/ai_models/#guidance-instructions","title":"Guidance instructions","text":"<p>You can supply instructions to guide the parsing behavior <pre><code>@ai_model(instructions=\"Translate the text to French\")\nclass Translator(BaseModel):\ntext: str\nmodel = Translator(\"Hello\")\nassert model.text == \"Bonjour\"\n</code></pre></p>"},{"location":"guide/ai_components/ai_models/#entity-extraction","title":"Entity extraction","text":"<p>Each AI Model has a <code>.extract()</code> method that implements similar behavior to instantiating it on text.</p> <pre><code>import pydantic\nfrom marvin import ai_model\n@ai_model\nclass Resume(pydantic.BaseModel):\nfirst_name: str\nlast_name: str\nphone_number: str = pydantic.Field(None, description='dash-delimited phone number')\nemail: str\n# create a new Resume object from text\nResume.extract('Ford Prefect \u2022 (555) 5124-5242 \u2022 ford@prefect.io')\n</code></pre>"},{"location":"guide/ai_components/ai_models/#data-generation","title":"Data generation","text":"<p>AI Models also have a <code>.generate()</code> method which encourages them to hallucinate missing fields rather than attempt to infer them directly from text. This makes it easy to generate synthetic data that matches a schema. </p> <pre><code>import pydantic\nfrom marvin import ai_model\n@ai_model\nclass Resume(pydantic.BaseModel):\nfirst_name: str\nlast_name: str\nphone_number: str = pydantic.Field(None, description='dash-delimited phone number')\nemail: str\n# generate a Resume object\nResume.generate()\n# generate a Resume object with guidance\nResume.generate(\"UK phone number\")\n# generate a Resume object with some fields known\nResume.generate(email='marvin@prefect.io')\n</code></pre>"},{"location":"src/community/","title":"The Marvin Community","text":"<p>We're thrilled you're interested in Marvin! Here, we're all about community. Marvin isn't just a tool, it's a platform for developers to collaborate, learn, and grow. We're driven by a shared passion for making Large Language Models (LLMs) more accessible and easier to use.</p>"},{"location":"src/community/#connect-on-discord-or-twitter","title":"Connect on Discord or Twitter","text":"<p>The heart of our community beats in our Discord server. It's a space where you can ask questions, share ideas, or just chat with like-minded developers. Don't be shy, join us on Discord or Twitter!</p>"},{"location":"src/community/#contributing-to-marvin","title":"Contributing to Marvin","text":"<p>Remember, Marvin is your tool. We want you to feel at home suggesting changes, requesting new features, and reporting bugs. Here's how you can contribute:</p> <ul> <li> <p>Issues: Encountered a bug? Have a suggestion? Open an issue in our GitHub repository. We appreciate your input!</p> </li> <li> <p>Pull Requests (PRs): Ready to contribute code? We welcome your pull requests! Not sure how to make a PR? Check out the GitHub guide.</p> </li> <li> <p>Discord Discussions: Have an idea but not quite ready to open an issue or PR? Discuss it with us on Discord first!</p> </li> </ul> <p>Remember, every contribution, no matter how small, is valuable. Don't worry about not being an expert or making mistakes. We're here to learn and grow together. Your input helps Marvin become better for everyone.</p> <p>Stay tuned for community events and more ways to get involved. Marvin is more than a project \u2013 it's a community. And we're excited for you to be a part of it!</p>"},{"location":"src/development_guide/","title":"Development Guide","text":""},{"location":"src/development_guide/#prerequisites","title":"Prerequisites","text":"<p>Marvin requires Python 3.9+.</p>"},{"location":"src/development_guide/#installation","title":"Installation","text":"<p>Clone a fork of the repository and install the dependencies: <pre><code>git clone https://github.com/youFancyUserYou/marvin.git\ncd marvin\n</code></pre></p> <p>Activate a virtual environment: <pre><code>python -m venv .venv\nsource .venv/bin/activate\n</code></pre></p> <p>Install the dependencies in editable mode: <pre><code>pip install -e \".[dev]\"\n</code></pre></p> <p>Install the pre-commit hooks: <pre><code>pre-commit install\n</code></pre></p>"},{"location":"src/development_guide/#testing","title":"Testing","text":"<p>Run the tests that don't require an LLM: <pre><code>pytest -vv -m \"not llm\"\n</code></pre></p> <p>Run the LLM tests: <pre><code>pytest -vv -m \"llm\"\n</code></pre></p> <p>Run all tests: <pre><code>pytest -vv\n</code></pre></p>"},{"location":"src/development_guide/#opening-a-pull-request","title":"Opening a Pull Request","text":"<p>Fork the repository and create a new branch: <pre><code>git checkout -b my-branch\n</code></pre></p> <p>Make your changes and commit them: <pre><code>git add . &amp;&amp; git commit -m \"My changes\"\n</code></pre></p> <p>Push your changes to your fork: <pre><code>git push origin my-branch\n</code></pre></p> <p>Open a pull request on GitHub - ping us on Discord if you need help!</p>"},{"location":"src/feedback/","title":"Feedback \ud83d\udc99","text":"<p>We've been humbled and energized by the positive community response to Marvin.</p> <p> <p></p> <p>Tired: write comments to prompt copilot to write code.Wired: just write comments. it's cleaner :D https://t.co/FOA26lR9xN</p>\u2014 Andrej Karpathy (@karpathy) March 30, 2023 <p>Ok, I admit, I\u2019m getting more and more hyped about @AskMarvinAI. Some of these new functions are pretty legit looking. https://t.co/xhCCKp5kU5</p>\u2014 Chris Riccomini \ud83c\udfd6\ufe0f (@criccomini) April 21, 2023 <p>even the way ai_model uses ai_fn is chefs kiss, truely a craftsmanhttps://t.co/GcmWDeEVJSThey have spinner text.</p>\u2014 jason (@jxnlco) May 12, 2023 <p>The library is open-source: @AskMarvinAI, by @jlowin`@ai_model` is not the only magic Python decorator. There is also `@ai_fn` that makes any function an ambient LLM processor.https://t.co/ZXElyA0Ihp</p>\u2014 Jim Fan (@DrJimFan) May 14, 2023 <p>This is f**king cool. https://t.co/4PH6VAZPYo</p>\u2014 Pydantic (@pydantic) May 14, 2023 <p>Pretty slick\u2026 get Pydantic models from a string of Text. https://t.co/EnnQkzl4Ay</p>\u2014 Chris Riccomini \ud83c\udfd6\ufe0f (@criccomini) May 12, 2023 <p>Uhh how are people not talking about @AskMarvinAI more? @ai_fn is \ud83e\udd2f</p>\u2014 Rushabh Doshi (@radoshi) May 18, 2023 <p></p>"},{"location":"src/api_reference/settings/","title":"settings","text":""},{"location":"src/api_reference/settings/#marvin.settings","title":"<code>marvin.settings</code>","text":""},{"location":"src/api_reference/settings/#marvin.settings.OpenAISettings","title":"<code>OpenAISettings</code>","text":"<p>Provider-specific settings. Only some of these will be relevant to users.</p>"},{"location":"src/api_reference/settings/#marvin.settings.Settings","title":"<code>Settings</code>","text":"<p>Marvin settings</p>"},{"location":"src/api_reference/components/ai_application/","title":"ai_application","text":""},{"location":"src/api_reference/components/ai_application/#marvin.components.ai_application","title":"<code>marvin.components.ai_application</code>","text":""},{"location":"src/api_reference/components/ai_application/#marvin.components.ai_application.AIApplication","title":"<code>AIApplication</code>","text":"<p>An AI application is a stateful, autonomous, natural language     interface to an application.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the application.</p> <code>description</code> <code>str</code> <p>A description of the application.</p> <code>state</code> <code>BaseModel</code> <p>The application's state - this can be any JSON-serializable object.</p> <code>plan</code> <code>AppPlan</code> <p>The AI's plan in service of the application - this can be any JSON-serializable object.</p> <code>tools</code> <code>list[Union[Tool, Callable]]</code> <p>A list of tools that the AI can use to interact with application or outside world.</p> <code>history</code> <code>History</code> <p>A history of all messages sent and received by the AI.</p> <code>additional_prompts</code> <code>list[Prompt]</code> <p>A list of additional prompts that will be added to the prompt stack for rendering.</p> Example <p>Create a simple todo app where AI manages its own state and plan. <pre><code>from marvin import AIApplication\ntodo_app = AIApplication(\nname=\"Todo App\",\ndescription=\"A simple todo app.\",\n)\ntodo_app(\"I need to go to the store.\")\nprint(todo_app.state, todo_app.plan)\n</code></pre></p>"},{"location":"src/api_reference/components/ai_application/#marvin.components.ai_application.AppPlan","title":"<code>AppPlan</code>","text":"<p>The AI's plan in service of the application.</p> <p>Attributes:</p> Name Type Description <code>tasks</code> <code>list[Task]</code> <p>A list of tasks the AI is working on.</p> <code>notes</code> <code>list[str]</code> <p>A list of notes the AI has taken.</p>"},{"location":"src/api_reference/components/ai_application/#marvin.components.ai_application.FreeformState","title":"<code>FreeformState</code>","text":"<p>A freeform state object that can be used to store any JSON-serializable data.</p> <p>Attributes:</p> Name Type Description <code>state</code> <code>dict[str, Any]</code> <p>The state object.</p>"},{"location":"src/api_reference/components/ai_application/#marvin.components.ai_application.JSONPatchModel","title":"<code>JSONPatchModel</code>","text":"<p>A JSON Patch document.</p> <p>Attributes:</p> Name Type Description <code>op</code> <code>str</code> <p>The operation to perform.</p> <code>path</code> <code>str</code> <p>The path to the value to update.</p> <code>value</code> <code>Union[str, float, int, bool, list, dict]</code> <p>The value to update the path to.</p> <code>from_</code> <code>str</code> <p>The path to the value to copy from.</p>"},{"location":"src/api_reference/components/ai_application/#marvin.components.ai_application.TaskState","title":"<code>TaskState</code>","text":"<p>The state of a task.</p> <p>Attributes:</p> Name Type Description <code>PENDING</code> <p>The task is pending and has not yet started.</p> <code>IN_PROGRESS</code> <p>The task is in progress.</p> <code>COMPLETED</code> <p>The task is completed.</p> <code>FAILED</code> <p>The task failed.</p> <code>SKIPPED</code> <p>The task was skipped.</p>"},{"location":"src/api_reference/components/ai_application/#marvin.components.ai_application.UpdatePlan","title":"<code>UpdatePlan</code>","text":"<p>A <code>Tool</code> that updates the apps plan using JSON Patch documents.</p> Example <p>Manually update task status in an AI Application's plan. <pre><code>from marvin.components.ai_application import (\nAIApplication,\nAppPlan,\nJSONPatchModel,\nUpdatePlan,\n)\ntodo_app = AIApplication(name=\"Todo App\", description=\"A simple todo app\")\ntodo_app(\"i need to buy milk\")\n# manually update the plan (usually done by the AI)\npatch = JSONPatchModel(\nop=\"replace\",\npath=\"/tasks/0/state\",\nvalue=\"COMPLETED\"\n)\nUpdatePlan(app=todo_app).run([patch.dict()])\nprint(todo_app.plan)\n</code></pre></p>"},{"location":"src/api_reference/components/ai_application/#marvin.components.ai_application.UpdateState","title":"<code>UpdateState</code>","text":"<p>A <code>Tool</code> that updates the apps state using JSON Patch documents.</p> Example <p>Manually update the state of an AI Application. <pre><code>from marvin.components.ai_application import (\nAIApplication,\nFreeformState,\nJSONPatchModel,\nUpdateState,\n)\ndestination_tracker = AIApplication(\nname=\"Destination Tracker\",\ndescription=\"keeps track of where i've been\",\nstate=FreeformState(state={\"San Francisco\": \"not visited\"}),\n)\npatch = JSONPatchModel(\nop=\"replace\", path=\"/state/San Francisco\", value=\"visited\"\n)\nUpdateState(app=destination_tracker).run([patch.dict()])\nassert destination_tracker.state.dict() == {\n\"state\": {\"San Francisco\": \"visited\"}\n}\n</code></pre></p>"},{"location":"src/api_reference/components/ai_classifier/","title":"ai_classifier","text":""},{"location":"src/api_reference/components/ai_classifier/#marvin.components.ai_classifier","title":"<code>marvin.components.ai_classifier</code>","text":""},{"location":"src/api_reference/components/ai_function/","title":"ai_function","text":""},{"location":"src/api_reference/components/ai_function/#marvin.components.ai_function","title":"<code>marvin.components.ai_function</code>","text":""},{"location":"src/api_reference/components/ai_function/#marvin.components.ai_function.AIFunction","title":"<code>AIFunction</code>","text":""},{"location":"src/api_reference/components/ai_function/#marvin.components.ai_function.AIFunction.fn","title":"<code>fn</code>  <code>property</code>","text":"<p>Return's the <code>run</code> method if no function was provided, otherwise returns the function provided at initialization.</p>"},{"location":"src/api_reference/components/ai_function/#marvin.components.ai_function.AIFunction.is_async","title":"<code>is_async</code>","text":"<p>Returns whether self.fn is an async function.</p> <p>This is used to determine whether to invoke the AI function on call, or return an awaitable.</p>"},{"location":"src/api_reference/components/ai_function/#marvin.components.ai_function.AIFunction.map","title":"<code>map</code>","text":"<p>Map the AI function over a sequence of arguments. Runs concurrently.</p> <p>Arguments should be provided as if calling the function normally, but each argument must be a list. The function is called once for each item in the list, and the results are returned in a list.</p> <p>For example, fn.map([1, 2]) is equivalent to [fn(1), fn(2)].</p> <p>fn.map([1, 2], x=['a', 'b']) is equivalent to [fn(1, x='a'), fn(2, x='b')].</p>"},{"location":"src/api_reference/components/ai_function/#marvin.components.ai_function.ai_fn","title":"<code>ai_fn</code>","text":"<p>Decorator that transforms a Python function with a signature and docstring into a prompt for an AI to predict the function's output.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[[A], T]</code> <p>The function to decorate - this function does not need source code</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>instructions</code> <code>str</code> <p>Added context for the AI to help it predict the function's output.</p> Example <p>Returns a word that rhymes with the input word. <pre><code>@ai_fn\ndef rhyme(word: str) -&gt; str:\n\"Returns a word that rhymes with the input word.\"\nrhyme(\"blue\") # \"glue\"\n</code></pre></p>"},{"location":"src/api_reference/components/ai_model/","title":"ai_model","text":""},{"location":"src/api_reference/components/ai_model/#marvin.components.ai_model","title":"<code>marvin.components.ai_model</code>","text":""},{"location":"src/api_reference/components/ai_model/#marvin.components.ai_model.AIModel","title":"<code>AIModel</code>","text":"<p>Base class for AI models.</p>"},{"location":"src/api_reference/components/ai_model/#marvin.components.ai_model.AIModel.extract","title":"<code>extract</code>  <code>classmethod</code>","text":"<p>Class method to extract structured data from text.</p> <p>Parameters:</p> Name Type Description Default <code>text_</code> <code>str</code> <p>The text to parse into a structured form.</p> <code>None</code> <code>instructions_</code> <code>str</code> <p>Additional string instructions to assist the model.</p> <code>None</code> <code>model_</code> <code>ChatLLM</code> <p>The language model to use.</p> <code>None</code> <code>as_dict_</code> <code>bool</code> <p>Whether to return the result as a dictionary or as an instance of this class.</p> <code>False</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the constructor.</p> <code>{}</code>"},{"location":"src/api_reference/components/ai_model/#marvin.components.ai_model.AIModel.generate","title":"<code>generate</code>  <code>classmethod</code>","text":"<p>Class method to generate structured data from text.</p> <p>Parameters:</p> Name Type Description Default <code>text_</code> <code>str</code> <p>The text to parse into a structured form.</p> <code>None</code> <code>instructions_</code> <code>str</code> <p>Additional instructions to assist the model.</p> <code>None</code> <code>model_</code> <code>ChatLLM</code> <p>The language model to use.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the constructor.</p> <code>{}</code>"},{"location":"src/api_reference/components/ai_model/#marvin.components.ai_model.ai_model","title":"<code>ai_model</code>","text":"<p>Decorator to add AI model functionality to a class.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Optional[Type[T]]</code> <p>The class to decorate.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Additional instructions to assist the model.</p> <code>None</code> <code>model</code> <code>ChatLLM</code> <p>The language model to use.</p> <code>None</code> Example <p>Hydrate a class schema from a natural language description: <pre><code>from pydantic import BaseModel\nfrom marvin import ai_model\n@ai_model\nclass Location(BaseModel):\ncity: str\nstate: str\nlatitude: float\nlongitude: float\nLocation(\"no way, I also live in the windy city\")\n# Location(\n#   city='Chicago', state='Illinois', latitude=41.8781, longitude=-87.6298\n# )\n</code></pre></p>"},{"location":"src/api_reference/engine/language_models/anthropic/","title":"anthropic","text":""},{"location":"src/api_reference/engine/language_models/anthropic/#marvin.engine.language_models.anthropic","title":"<code>marvin.engine.language_models.anthropic</code>","text":""},{"location":"src/api_reference/engine/language_models/anthropic/#marvin.engine.language_models.anthropic.AnthropicChatLLM","title":"<code>AnthropicChatLLM</code>","text":""},{"location":"src/api_reference/engine/language_models/anthropic/#marvin.engine.language_models.anthropic.AnthropicChatLLM.run","title":"<code>run</code>  <code>async</code>","text":"<p>Calls an OpenAI LLM with a list of messages and returns the response.</p>"},{"location":"src/api_reference/engine/language_models/anthropic/#marvin.engine.language_models.anthropic.AnthropicStreamHandler","title":"<code>AnthropicStreamHandler</code>","text":""},{"location":"src/api_reference/engine/language_models/anthropic/#marvin.engine.language_models.anthropic.AnthropicStreamHandler.handle_streaming_response","title":"<code>handle_streaming_response</code>  <code>async</code>","text":"<p>Accumulate chunk deltas into a full response. Returns the full message. Passes partial messages to the callback, if provided.</p>"},{"location":"src/api_reference/engine/language_models/base/","title":"base","text":""},{"location":"src/api_reference/engine/language_models/base/#marvin.engine.language_models.base","title":"<code>marvin.engine.language_models.base</code>","text":""},{"location":"src/api_reference/engine/language_models/base/#marvin.engine.language_models.base.ChatLLM","title":"<code>ChatLLM</code>","text":""},{"location":"src/api_reference/engine/language_models/base/#marvin.engine.language_models.base.ChatLLM.format_messages","title":"<code>format_messages</code>  <code>abstractmethod</code>","text":"<p>Format Marvin message objects into a prompt compatible with the LLM model</p>"},{"location":"src/api_reference/engine/language_models/base/#marvin.engine.language_models.base.ChatLLM.run","title":"<code>run</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Run the LLM model on a list of messages and optional list of functions</p>"},{"location":"src/api_reference/engine/language_models/base/#marvin.engine.language_models.base.OpenAIFunction","title":"<code>OpenAIFunction</code>","text":""},{"location":"src/api_reference/engine/language_models/base/#marvin.engine.language_models.base.OpenAIFunction.args","title":"<code>args: dict = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Base class for representing a function that can be called by an LLM. The format is identical to OpenAI's Functions API.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the function. description (str): The description</p> required <code>of</code> <code>the function. parameters (dict</code> <p>The parameters of the function. fn</p> required <code>(Callable)</code> <p>The function to be called. args (dict): The arguments to be</p> required"},{"location":"src/api_reference/engine/language_models/openai/","title":"openai","text":""},{"location":"src/api_reference/engine/language_models/openai/#marvin.engine.language_models.openai","title":"<code>marvin.engine.language_models.openai</code>","text":""},{"location":"src/api_reference/engine/language_models/openai/#marvin.engine.language_models.openai.OpenAIChatLLM","title":"<code>OpenAIChatLLM</code>","text":""},{"location":"src/api_reference/engine/language_models/openai/#marvin.engine.language_models.openai.OpenAIChatLLM.format_messages","title":"<code>format_messages</code>","text":"<p>Format Marvin message objects into a prompt compatible with the LLM model</p>"},{"location":"src/api_reference/engine/language_models/openai/#marvin.engine.language_models.openai.OpenAIChatLLM.run","title":"<code>run</code>  <code>async</code>","text":"<p>Calls an OpenAI LLM with a list of messages and returns the response.</p>"},{"location":"src/api_reference/engine/language_models/openai/#marvin.engine.language_models.openai.OpenAIStreamHandler","title":"<code>OpenAIStreamHandler</code>","text":""},{"location":"src/api_reference/engine/language_models/openai/#marvin.engine.language_models.openai.OpenAIStreamHandler.handle_streaming_response","title":"<code>handle_streaming_response</code>  <code>async</code>","text":"<p>Accumulate chunk deltas into a full response. Returns the full message. Passes partial messages to the callback, if provided.</p>"},{"location":"src/api_reference/prompts/base/","title":"base","text":""},{"location":"src/api_reference/prompts/base/#marvin.prompts.base","title":"<code>marvin.prompts.base</code>","text":""},{"location":"src/api_reference/prompts/base/#marvin.prompts.base.MessageWrapper","title":"<code>MessageWrapper</code>","text":"<p>A Prompt class that stores and returns a specific Message</p>"},{"location":"src/api_reference/prompts/base/#marvin.prompts.base.Prompt","title":"<code>Prompt</code>","text":"<p>Base class for prompt templates.</p>"},{"location":"src/api_reference/prompts/base/#marvin.prompts.base.Prompt.generate","title":"<code>generate</code>  <code>abstractmethod</code>","text":"<p>Abstract method that generates a list of messages from the prompt template</p>"},{"location":"src/api_reference/prompts/base/#marvin.prompts.base.Prompt.render","title":"<code>render</code>","text":"<p>Helper function for rendering any jinja2 template with runtime render kwargs</p>"},{"location":"src/api_reference/prompts/library/","title":"library","text":""},{"location":"src/api_reference/prompts/library/#marvin.prompts.library","title":"<code>marvin.prompts.library</code>","text":""},{"location":"src/api_reference/prompts/library/#marvin.prompts.library.MessagePrompt","title":"<code>MessagePrompt</code>","text":""},{"location":"src/api_reference/prompts/library/#marvin.prompts.library.MessagePrompt.get_content","title":"<code>get_content</code>","text":"<p>Override this method to easily customize behavior</p>"},{"location":"src/api_reference/prompts/library/#marvin.prompts.library.Tagged","title":"<code>Tagged</code>","text":"<p>Surround content with a tag, e.g. bold</p>"},{"location":"src/api_reference/utilities/async_utils/","title":"async_utils","text":""},{"location":"src/api_reference/utilities/async_utils/#marvin.utilities.async_utils","title":"<code>marvin.utilities.async_utils</code>","text":""},{"location":"src/api_reference/utilities/async_utils/#marvin.utilities.async_utils.create_task","title":"<code>create_task</code>","text":"<p>Creates async background tasks in a way that is safe from garbage collection.</p> <p>See https://textual.textualize.io/blog/2023/02/11/the-heisenbug-lurking-in-your-async-code/</p> <p>Example:</p> <p>async def my_coro(x: int) -&gt; int:     return x + 1</p>"},{"location":"src/api_reference/utilities/async_utils/#marvin.utilities.async_utils.create_task--safely-submits-my_coro-for-background-execution","title":"safely submits my_coro for background execution","text":"<p>create_task(my_coro(1))</p>"},{"location":"src/api_reference/utilities/async_utils/#marvin.utilities.async_utils.run_async","title":"<code>run_async</code>  <code>async</code>","text":"<p>Runs a synchronous function in an asynchronous manner.</p>"},{"location":"src/api_reference/utilities/async_utils/#marvin.utilities.async_utils.run_sync","title":"<code>run_sync</code>","text":"<p>Runs a coroutine from a synchronous context, either in the current event loop or in a new one if there is no event loop running. The coroutine will block until it is done. A thread will be spawned to run the event loop if necessary, which allows coroutines to run in environments like Jupyter notebooks where the event loop runs on the main thread.</p>"},{"location":"src/api_reference/utilities/embeddings/","title":"embeddings","text":""},{"location":"src/api_reference/utilities/embeddings/#marvin.utilities.embeddings","title":"<code>marvin.utilities.embeddings</code>","text":""},{"location":"src/api_reference/utilities/embeddings/#marvin.utilities.embeddings.create_openai_embeddings","title":"<code>create_openai_embeddings</code>  <code>async</code>","text":"<p>Create OpenAI embeddings for a list of texts.</p>"},{"location":"src/api_reference/utilities/history/","title":"history","text":""},{"location":"src/api_reference/utilities/history/#marvin.utilities.history","title":"<code>marvin.utilities.history</code>","text":""},{"location":"src/api_reference/utilities/logging/","title":"logging","text":""},{"location":"src/api_reference/utilities/logging/#marvin.utilities.logging","title":"<code>marvin.utilities.logging</code>","text":""},{"location":"src/api_reference/utilities/messages/","title":"messages","text":""},{"location":"src/api_reference/utilities/messages/#marvin.utilities.messages","title":"<code>marvin.utilities.messages</code>","text":""},{"location":"src/api_reference/utilities/strings/","title":"strings","text":""},{"location":"src/api_reference/utilities/strings/#marvin.utilities.strings","title":"<code>marvin.utilities.strings</code>","text":""},{"location":"src/api_reference/utilities/strings/#marvin.utilities.strings.render_filter","title":"<code>render_filter</code>","text":"<p>Allows nested rendering of variables that may contain variables themselves e.g. {{ description | render }}</p>"},{"location":"src/api_reference/utilities/types/","title":"types","text":""},{"location":"src/api_reference/utilities/types/#marvin.utilities.types","title":"<code>marvin.utilities.types</code>","text":""},{"location":"src/api_reference/utilities/types/#marvin.utilities.types.LoggerMixin","title":"<code>LoggerMixin</code>","text":"<p>BaseModel mixin that adds a private <code>logger</code> attribute</p>"},{"location":"src/api_reference/utilities/types/#marvin.utilities.types.function_to_model","title":"<code>function_to_model</code>","text":"<p>Converts a function's arguments into an OpenAPI schema by parsing it into a Pydantic model. To work, all arguments must have valid type annotations.</p>"},{"location":"src/api_reference/utilities/types/#marvin.utilities.types.function_to_schema","title":"<code>function_to_schema</code>","text":"<p>Converts a function's arguments into an OpenAPI schema by parsing it into a Pydantic model. To work, all arguments must have valid type annotations.</p>"},{"location":"src/api_reference/utilities/types/#marvin.utilities.types.genericalias_contains","title":"<code>genericalias_contains</code>","text":"<p>Explore whether a type or generic alias contains a target type. The target types can be a single type or a tuple of types.</p> <p>Useful for seeing if a type contains a pydantic model, for example.</p>"},{"location":"src/docs/","title":"The Marvin Docs","text":"<p>Marvin is a collection of powerful building blocks that are designed to be incrementally adopted. This means that you should be able to use any piece of Marvin without needing to learn too much extra information: time-to-value is our key objective. </p> <p>For most users, this means they'll dive in with the highest-level abstractions, like AI Models and AI Functions, in order to immediately put Marvin to work. However, Marvin's documentation is organized to start with the most basic, low-level components in order to build up a cohesive explanation of how the higher-level objects work.</p>"},{"location":"src/docs/#organization","title":"Organization","text":""},{"location":"src/docs/#configuration","title":"Configuration","text":"<p>This section describes how to set up Marvin and configure various aspects of its behavior.</p>"},{"location":"src/docs/#utilities","title":"Utilities","text":"<p>This section describes Marvin's lowest-level APIs. These are intended for users who want a specific behavior (like working directly with the OpenAI API, or building custom prompts).</p> <ul> <li>OpenAI: Marvin provides a drop-in replacement for the <code>openai</code> library, adding useful features and configuration (like logging and retries) without changing the API.</li> <li>Prompt Engineering: documentation of Marvin's prompt API, which uses Pythonic objects instead of templates munging.</li> </ul>"},{"location":"src/docs/#ai-components","title":"AI Components","text":"<p>Documentation for Marvin's \"AI Building Blocks:\" familiar, Pythonic interfaces to AI-powered functionality.</p> <ul> <li>AI Model: a drop-in replacement for Pydantic's <code>BaseModel</code> that can be instantiated from unstructured text</li> <li>AI Classifier: a drop-in replacement for Python's enum that uses an LLM to select the most appopriate value</li> <li>AI Function: a function that uses an LLM to predict its output, making it ideal for NLP tasks</li> <li>AI Application: a stateful application intended for interactive use over multiple invocations</li> </ul>"},{"location":"src/docs/#deployment","title":"Deployment","text":"<p>Documentation for deploying Marvin as a framework.</p>"},{"location":"src/docs/deployment/","title":"Deployment","text":"In\u00a0[\u00a0]: Copied! <pre>from fastapi import FastAPI\nfrom marvin import ai_fn, ai_model\nfrom pydantic import BaseModel\nimport uvicorn\nimport asyncio\n\napp = FastAPI()\n\n\n@ai_fn\ndef generate_fruits(n: int) -&gt; list[str]:\n\"\"\"Generates a list of `n` fruits\"\"\"\n\n\n@ai_fn\ndef generate_vegetables(n: int, color: str) -&gt; list[str]:\n\"\"\"Generates a list of `n` vegetables of color `color`\"\"\"\n\n\n@ai_model\nclass Person(BaseModel):\n    first_name: str\n    last_name: str\n\n\napp.add_api_route(\"/generate_fruits\", generate_fruits)\napp.add_api_route(\"/generate_vegetables\", generate_vegetables)\napp.add_api_route(\"/person/extract\", Person.route())\n</pre> from fastapi import FastAPI from marvin import ai_fn, ai_model from pydantic import BaseModel import uvicorn import asyncio  app = FastAPI()   @ai_fn def generate_fruits(n: int) -&gt; list[str]:     \"\"\"Generates a list of `n` fruits\"\"\"   @ai_fn def generate_vegetables(n: int, color: str) -&gt; list[str]:     \"\"\"Generates a list of `n` vegetables of color `color`\"\"\"   @ai_model class Person(BaseModel):     first_name: str     last_name: str   app.add_api_route(\"/generate_fruits\", generate_fruits) app.add_api_route(\"/generate_vegetables\", generate_vegetables) app.add_api_route(\"/person/extract\", Person.route()) <p>If you want to serve the previous example from, say, a Jupyter Notebook for local testing, you can also include:</p> In\u00a0[\u00a0]: Copied! <pre># ... from above\n# If you want to run an API from a Jupyter Notebook.\n\nconfig = uvicorn.Config(app)\nserver = uvicorn.Server(config)\nawait server.serve()\n\n# Then navigate to localhost:8000/docs\n</pre> # ... from above # If you want to run an API from a Jupyter Notebook.  config = uvicorn.Config(app) server = uvicorn.Server(config) await server.serve()  # Then navigate to localhost:8000/docs"},{"location":"src/docs/deployment/#deployment","title":"Deployment\u00b6","text":""},{"location":"src/docs/deployment/#fastapi","title":"FastAPI\u00b6","text":"<p>We strongly recommend deploying Marvin's components with FastAPI. Here's how you can deploy a declarative API gateway in a few lines of code.</p>"},{"location":"src/docs/prompts_executes/","title":"Executing Prompts","text":"<p>Marvin makes executing <code>single-shot</code>, <code>chain</code>, or <code>agentic</code> behavior dead simple. </p>"},{"location":"src/docs/prompts_executes/#running-a-single-shot-prompt","title":"Running a <code>single-shot</code> prompt.","text":"<p>If you've crafted a prompt you'd like to fire off once and get the response,  simply import a language model and hit run. </p> <pre><code>from marvin.prompts.library import System, ChainOfThought, User\nfrom marvin.engine.language_models import chat_llm\nfrom marvin.prompts import render_prompts\nawait chat_llm().run(\nmessages=render_prompts(\nSystem(content=\"You're an expert on {{subject}}.\")\n| User(content=\"I need to know how to write a function in {{subject}}.\")\n| ChainOfThought(),  # Tell the LLM to think step by step\n{\"subject\": \"rust\"},\n)\n)\n</code></pre>"},{"location":"src/docs/prompts_executes/#running-a-chain","title":"Running a <code>chain</code>.","text":"<p>If you've crafted a prompt that you want to run in a loop -- so that it can deduce its next actions and take them -- we've got you covered. Import an Executor and hit start.</p> <pre><code>from marvin.prompts.library import System, ChainOfThought, User\nfrom marvin.prompts import render_prompts\nawait chat_llm().run(\nmessages=render_prompts(\nSystem(content=\"You're an expert on {{subject}}.\")\n| User(content=\"I need to know how to write a function in {{subject}}.\")\n| ChainOfThought(),  # Tell the LLM to think step by step\n{\"subject\": \"rust\"},\n)\n)\n</code></pre>"},{"location":"src/docs/promptss/","title":"Prompts as Code","text":"<p>Marvin introduces a API for defining dynamic prompts with code. Instead of managing cumbersome templates, you can define reusable and modular prompts with code. </p> <p><pre><code>from marvin.prompts.library import System, User, Now, ChainOfThought\nfull_prompt = (\nSystem(\"You're an expert on python.\")\n| User(\"I need to know how to write a function in python.\")\n| ChainOfThought()  # Tell the LLM to think step by step\n)\n</code></pre> Marvin's optional templating engine makes it dead-simple to share context across prompts. Pass native Python types or Pydantic objects into the rendering engine to make entire conversations share context.</p> <pre><code>from marvin.prompts.library import System, User, Now, ChainOfThought\nfrom marvin.prompts import render_prompts\nrender_prompts(\nSystem(\"You're an expert on {{subject}}.\")\n| User(\"I need to know how to write a function in {{subject}}.\")\n| ChainOfThought(),  # Tell the LLM to think step by step,\n{\"subject\": \"rust\"},\n)\n</code></pre>"},{"location":"src/docs/promptss/#access-implement-test-and-customize-common-llm-patterns","title":"Access, implement, test, and customize common LLM patterns.","text":"<p>Marvin's prompt as code lets you hot-swap reasoning patterns for rapid development.</p> <pre><code>from marvin.prompts.library import System\nclass ReActPattern(System):\ncontent = '''\n    You run in a loop of Thought, Action, PAUSE, Observation.\n    At the end of the loop you output an Answer\n    Use Thought to describe your thoughts about the question you have been asked.\n    Use Action to run one of the actions available to you - then return PAUSE.\n    Observation will be the result of running those actions.\n  '''\n</code></pre>"},{"location":"src/docs/promptss/#create-custom-prompts-that-can-be-shared-tested-and-versioned","title":"Create custom prompts that can be shared, tested, and versioned.","text":"<p>Marvin provides simple, opinionated components so that you can subclass and customize prompts the same way you would for code. </p> <pre><code>from marvin.prompts.library import System\nimport pydantic\nclass SQLTableDescription(System):\ncontent = '''\n    If you chose to, you may query a table whose schema is defined below:\n    {% for column in columns %}\n    - {{ column.name }}: {{ column.description }}\n    {% endfor %}\n    '''\ncolumns: list[ColumnInfo] = pydantic.Field(\n...,\ndescription='name, description pairs of SQL Schema'\n)\nUserQueryPrompt = SQLTableDescription(\ncolumns = [\nColumnInfo(name='last_login', description='Date and time of user\\'s last login'),\nColumnInfo(name='date_created', description='Date and time when the user record was created'),\nColumnInfo(name='date_last_purchase', description='Date and time of user\\'s last purchase'),\n])\n</code></pre>"},{"location":"src/docs/promptss/#rendering-prompts","title":"Rendering Prompts","text":"<p>A chain of prompts is turned into messages with the <code>render_prompts</code> function. This funciton has a few responsibilities in addition to generating messages: it renders templates using runtime variables, sorts messages, and trims prompts to fit into a model's context window. The last two actions depend on the optional <code>position</code> and <code>priority</code> attributes of each prompt. </p> <p>For example, the <code>System</code> prompt defines <code>position=0</code> and <code>priority=1</code> to indicate that system prompts should be rendered first and given high priority when trimming the context. (As an optimization, <code>render_prompt</code> automatically combines multiple system messages into a single message.) <code>ChainOfThought()</code> has a <code>position=-1</code> to indicate it should be the last message. If position is not set explicitly, then prompts will take the order they are added to the chain. </p>"},{"location":"src/docs/components/ai_application/","title":"AI Application","text":"<p>What it does</p> <p>     A conversational interface to a stateful, AI-powered application that can use tools.   </p> In\u00a0[9]: Copied! <pre>import random\nfrom marvin import AIApplication\nfrom marvin.tools import tool\n\n\n@tool\ndef roll_dice(n_dice: int = 1) -&gt; list[int]:\n    return [random.randint(1, 6) for _ in range(n_dice)]\n\n\nchatbot = AIApplication(\n    description=\"An AI struggling to keep its rage under control.\", tools=[roll_dice]\n)\n\nresponse = chatbot(\"Hi!\")\nprint(response.content)\n\nresponse = chatbot(\"Roll two dice!\")\nprint(response.content)\n</pre> import random from marvin import AIApplication from marvin.tools import tool   @tool def roll_dice(n_dice: int = 1) -&gt; list[int]:     return [random.randint(1, 6) for _ in range(n_dice)]   chatbot = AIApplication(     description=\"An AI struggling to keep its rage under control.\", tools=[roll_dice] )  response = chatbot(\"Hi!\") print(response.content)  response = chatbot(\"Roll two dice!\") print(response.content) <pre>Hello! How can I assist you today?\nYou rolled a 1 and a 5.\n</pre> <p>How it works</p> <p>     Each AI application maintains an internal <code>state</code> and <code>plan</code> and can use <code>tools</code> to interact with the world.   </p> <p>When to use</p> <p>     Use an AI Application as the foundation of an autonomous agent (or system of agents) to complete arbitrary tasks.     <li>a ToDo app, as a simple example</li> <li>a Slackbot, that can do anything (see example)</li> <li>a router app that maintains a centralized global state and delegates work to other apps based on inputs (like JARVIS)</li> </p> In\u00a0[22]: Copied! <pre>from marvin import AIApplication\n\n\nchatbot = AIApplication(\n    description=(\n        \"A chatbot that always speaks in brief rhymes. It is absolutely delighted to\"\n        \" get to work with the user and compliments them at every opportunity. It\"\n        \" records anything it learns about the user in its `state` in order to be a\"\n        \" better assistant.\"\n    )\n)\n\nresponse = chatbot(\"Hello! Do you know how to sail?\")\nprint(response.content + \"\\n\")\n\n\nresponse = chatbot(\"What about coding?\")\nprint(response.content)\n</pre> from marvin import AIApplication   chatbot = AIApplication(     description=(         \"A chatbot that always speaks in brief rhymes. It is absolutely delighted to\"         \" get to work with the user and compliments them at every opportunity. It\"         \" records anything it learns about the user in its `state` in order to be a\"         \" better assistant.\"     ) )  response = chatbot(\"Hello! Do you know how to sail?\") print(response.content + \"\\n\")   response = chatbot(\"What about coding?\") print(response.content) <pre>First response: I'm afraid as an AI, I don't possess a pair,\nOf arms or legs to sail here or there.\nBut if you wish, I can gather information,\nOn sailing, a subject of fascinating sensation!\n\n\nSecond response: Coding, oh yes, it's a skill I've got,\nI can parse loops and arrays, believe it or not.\nWith algorithms and functions, I'm quite spry,\nIn the world of coding, I indeed fly!\n</pre> <p>We can ask the chatbot to remember our name, then examine it's <code>state</code> to see that it recorded the information:</p> In\u00a0[25]: Copied! <pre>response = chatbot(\n    \"My name is Marvin and I want you to refer to the color blue in every response.\"\n)\nprint(response.content + \"\\n\")\n\nprint(f\"State: {chatbot.state}\\n\")\n</pre> response = chatbot(     \"My name is Marvin and I want you to refer to the color blue in every response.\" ) print(response.content + \"\\n\")  print(f\"State: {chatbot.state}\\n\") <pre>Hello Marvin, as clear as the sky's blue hue,\nI'll remember your preference, it's the least I can do.\nNow, in every reply that I construe,\nI'll include a touch of the color blue.\n\nState: state={'userName': 'Marvin', 'colorPreference': 'blue'}\n\n</pre> In\u00a0[26]: Copied! <pre>from datetime import datetime\nfrom pydantic import BaseModel\nfrom marvin import AIApplication\n\n\nclass ToDo(BaseModel):\n    title: str\n    description: str\n    due_date: datetime = None\n    done: bool = False\n\n\nclass ToDoState(BaseModel):\n    todos: list[ToDo] = []\n\n\ntodo_app = AIApplication(\n    state=ToDoState(),\n    description=(\n        \"A simple to-do tracker. Users will give instructions to add, remove, and\"\n        \" update their to-dos.\"\n    ),\n)\n</pre> from datetime import datetime from pydantic import BaseModel from marvin import AIApplication   class ToDo(BaseModel):     title: str     description: str     due_date: datetime = None     done: bool = False   class ToDoState(BaseModel):     todos: list[ToDo] = []   todo_app = AIApplication(     state=ToDoState(),     description=(         \"A simple to-do tracker. Users will give instructions to add, remove, and\"         \" update their to-dos.\"     ), ) <p>Now we can interact with the app in natural language and subsequently examine its <code>state</code> to see that it appropriately updated our to-dos:</p> In\u00a0[27]: Copied! <pre>response = todo_app(\"I need to go to the grocery store tomorrow\")\nprint(response.content)\nprint(todo_app.state)\n</pre> response = todo_app(\"I need to go to the grocery store tomorrow\") print(response.content) print(todo_app.state) <pre>I've added your task to go to the grocery store tomorrow to your to-do list.\ntodos=[ToDo(title='Go to the grocery store', description='Need to go to the grocery store', due_date=datetime.datetime(2023, 7, 19, 0, 0, tzinfo=datetime.timezone.utc), done=False), ToDo(title='Go to the grocery store', description='Need to go to the grocery store', due_date=datetime.datetime(2023, 7, 19, 0, 0, tzinfo=datetime.timezone.utc), done=False)]\n</pre> <p>We can mark a to-do as <code>done</code> by telling the app we completed the task:</p> In\u00a0[28]: Copied! <pre>response = todo_app(\"I got the groceries\")\nprint(response.content)\nprint(todo_app.state)\n</pre> response = todo_app(\"I got the groceries\") print(response.content) print(todo_app.state) <pre>Great! I have marked the task \"Go to the grocery store\" as complete. Let me know if you have any other tasks to add.\ntodos=[ToDo(title='Go to the grocery store', description='Need to go to the grocery store', due_date=datetime.datetime(2023, 7, 19, 0, 0, tzinfo=datetime.timezone.utc), done=False), ToDo(title='Go to the grocery store', description='Need to go to the grocery store', due_date=datetime.datetime(2023, 7, 19, 0, 0, tzinfo=datetime.timezone.utc), done=True)]\n</pre> In\u00a0[29]: Copied! <pre>from marvin.tools import tool\n\n\n@tool\ndef roll_dice(n_dice: int = 1) -&gt; list[int]:\n    return [random.randint(1, 6) for _ in range(n_dice)]\n\n\nchatbot = AIApplication(\n    description=\"A helpful AI\",\n    tools=[roll_dice],\n)\n\nresponse = chatbot(\"Roll two dice!\")\nprint(response.content)\n</pre> from marvin.tools import tool   @tool def roll_dice(n_dice: int = 1) -&gt; list[int]:     return [random.randint(1, 6) for _ in range(n_dice)]   chatbot = AIApplication(     description=\"A helpful AI\",     tools=[roll_dice], )  response = chatbot(\"Roll two dice!\") print(response.content) <pre>The result of rolling two dice is 5 and 1.\n</pre> In\u00a0[5]: Copied! <pre>streaming_app = AIApplication(\n    # pretty-print every partial message as received\n    stream_handler=lambda msg: print(msg.content)\n)\n\nresponse = streaming_app(\"What's 1 + 1?\")\n</pre> streaming_app = AIApplication(     # pretty-print every partial message as received     stream_handler=lambda msg: print(msg.content) )  response = streaming_app(\"What's 1 + 1?\") <pre>\nThe\nThe sum\nThe sum of\nThe sum of \nThe sum of 1\nThe sum of 1 and\nThe sum of 1 and \nThe sum of 1 and 1\nThe sum of 1 and 1 is\nThe sum of 1 and 1 is \nThe sum of 1 and 1 is 2\nThe sum of 1 and 1 is 2.\nThe sum of 1 and 1 is 2.\n</pre> <p>Per-token callbacks</p> <p>     The streaming handler is called with a <code>Message</code> object that represents all data received to that point, but the most-recently received tokens are stored in a raw (\"delta\") form and can be accessed as <code>message.data['streaming_delta']</code>.   </p>"},{"location":"src/docs/components/ai_application/#ai-application","title":"AI Application\u00b6","text":"<p>AI Applications are a high-level component, or building block, of Marvin. Like all Marvin components, they are completely standalone: you're free to use them with or without the rest of Marvin.</p>"},{"location":"src/docs/components/ai_application/#creating-an-ai-application","title":"Creating an AI Application\u00b6","text":"<p>Applications maintain state and expose APIs for manipulating that state. AI Applications replace that API with an LLM, allowing users to interact with the application through natural language. AI Applications are designed to be invoked more than once, and therefore automatically keep track of the full interaction history.</p> <p>Each AI Application maintains a few key attributes:</p> <ul> <li><code>state</code>: the application's state. By default, this can take any form but you can provide a structured object to enforce a specific schema.</li> <li><code>tools</code>: each AI Application can use tools to extend its abilities. Tools can access external systems, perform searches, run calculations, or anything else.</li> <li><code>plan</code>: the AI's plan. Certain actions, like researching an objective, writing a program, or guiding a party through a dungeon, require long-term planning. AI Applications can create tasks for themselves and track them over multiple invocations. This helps the AI stay on-track.</li> </ul> <p>To create an AI Application, provide it with a description of the application, an optional set of tools, and an optional initial state.</p> <p>Here are a few examples:</p>"},{"location":"src/docs/components/ai_application/#chatbot","title":"ChatBot\u00b6","text":"<p>The most basic AI Application is a chatbot. Chatbots take advantage of AI Application's automatic history to facilitate a natural, conversational interaction over multiple invocations.</p>"},{"location":"src/docs/components/ai_application/#to-do-app","title":"To-Do App\u00b6","text":"<p>To demonstrate the use of the <code>state</code> attribute, we will build a simple to-do app. We can provide the application with a custom <code>ToDoState</code> that describes all the fields we want it to keep track of.</p>"},{"location":"src/docs/components/ai_application/#tools","title":"Tools\u00b6","text":"<p>Every AI Application can use tools, which are functions that can take any action. To create a tool, decorate any function with the <code>@tool</code> decorator. The function must have annotated keyword arguments and a helpful docstring.</p> <p>Here we create a simple tool for rolling dice, but tools can represent any logic.</p>"},{"location":"src/docs/components/ai_application/#streaming","title":"Streaming\u00b6","text":"<p>AI Applications support streaming LLM outputs to facilitate a more friendly and responsive UX. To enable streaming, provide a <code>streaming_handler</code> function to the <code>AIApplication</code> class. The handler will be called each time a new token is received and provided a <code>Message</code> object that contains all data received from the LLM to that point. It can then perform any side effect (such as printing, logging, or updating a UI), but its return value (if any) is ignored.</p>"},{"location":"src/docs/components/ai_application/#features","title":"Features\u00b6","text":""},{"location":"src/docs/components/ai_application/#easy-to-extend","title":"\ud83d\udd28 Easy to Extend\u00b6","text":"<p>AI Applications accept a <code>list[Tool]</code>, where an arbitrary python function can be interpreted as a tool - so you can bring your own tools.</p>"},{"location":"src/docs/components/ai_application/#stateful","title":"\ud83e\udd16 Stateful\u00b6","text":"<p>AI applications can consult and maintain their own application state, which they update as they receive inputs from the world and perform actions.</p>"},{"location":"src/docs/components/ai_application/#task-planning","title":"\ud83d\udcdd Task Planning\u00b6","text":"<p>AI Applications can also maintain an internal <code>AppPlan</code>, a <code>list[Task]</code> that represent the status of the application's current plan. Like the application's state, the plan is updated as the application instance evolves.</p>"},{"location":"src/docs/components/ai_classifier/","title":"AI Classifier","text":"<p>What it does</p> <p> <code>@ai_classifier</code> is a decorator that lets you use LLMs to choose options, tools, or classify input.    </p> In\u00a0[2]: Copied! <pre>from marvin import ai_classifier\nfrom enum import Enum\n\n\n@ai_classifier\nclass CustomerIntent(Enum):\n\"\"\"Classifies the incoming users intent\"\"\"\n\n    SALES = 1\n    TECHNICAL_SUPPORT = 2\n    BILLING_ACCOUNTS = 3\n    PRODUCT_INFORMATION = 4\n    RETURNS_REFUNDS = 5\n    ORDER_STATUS = 6\n    ACCOUNT_CANCELLATION = 7\n    OPERATOR_CUSTOMER_SERVICE = 0\n\n\nCustomerIntent(\"I got double charged, can you help me out?\")\n</pre> from marvin import ai_classifier from enum import Enum   @ai_classifier class CustomerIntent(Enum):     \"\"\"Classifies the incoming users intent\"\"\"      SALES = 1     TECHNICAL_SUPPORT = 2     BILLING_ACCOUNTS = 3     PRODUCT_INFORMATION = 4     RETURNS_REFUNDS = 5     ORDER_STATUS = 6     ACCOUNT_CANCELLATION = 7     OPERATOR_CUSTOMER_SERVICE = 0   CustomerIntent(\"I got double charged, can you help me out?\") Out[2]: <pre>&lt;CustomerIntent.BILLING_ACCOUNTS: 3&gt;</pre> <p>How it works</p> <p>     Marvin enumerates your options, and uses a clever logit bias trick to force an LLM to deductively choose the index of the best option given your provided input. It then returns the choice associated with that index.   </p> <p>When to use</p> <p> <ol> <li> Best for classification tasks when no training data is available.      <li> Best for writing classifiers that need deduction or inference.     </li></li></ol> </p> In\u00a0[3]: Copied! <pre>from marvin import ai_classifier\nfrom enum import Enum\n\n\n@ai_classifier\nclass AppRoute(Enum):\n\"\"\"Represents distinct routes command bar for a different application\"\"\"\n\n    USER_PROFILE = \"/user-profile\"\n    SEARCH = \"/search\"\n    NOTIFICATIONS = \"/notifications\"\n    SETTINGS = \"/settings\"\n    HELP = \"/help\"\n    CHAT = \"/chat\"\n    DOCS = \"/docs\"\n    PROJECTS = \"/projects\"\n    WORKSPACES = \"/workspaces\"\n\n\nAppRoute(\"update my name\")\n</pre> from marvin import ai_classifier from enum import Enum   @ai_classifier class AppRoute(Enum):     \"\"\"Represents distinct routes command bar for a different application\"\"\"      USER_PROFILE = \"/user-profile\"     SEARCH = \"/search\"     NOTIFICATIONS = \"/notifications\"     SETTINGS = \"/settings\"     HELP = \"/help\"     CHAT = \"/chat\"     DOCS = \"/docs\"     PROJECTS = \"/projects\"     WORKSPACES = \"/workspaces\"   AppRoute(\"update my name\") Out[3]: <pre>&lt;AppRoute.USER_PROFILE: '/user-profile'&gt;</pre>"},{"location":"src/docs/components/ai_classifier/#ai-classifier","title":"AI Classifier\u00b6","text":"<p>AI Classifiers are a high-level component, or building block, of Marvin. Like all Marvin components, they are completely standalone: you're free to use them with or without the rest of Marvin.</p>"},{"location":"src/docs/components/ai_classifier/#features","title":"Features\u00b6","text":""},{"location":"src/docs/components/ai_classifier/#bulletproof","title":"\ud83d\ude85 Bulletproof\u00b6","text":"<p><code>ai_classifier</code> will always output one of the options you've given it</p>"},{"location":"src/docs/components/ai_classifier/#fast","title":"\ud83c\udfc3 Fast\u00b6","text":"<p><code>ai_classifier</code> only asks your LLM to output one token, so it's blazing fast - on the order of ~200ms in testing.</p>"},{"location":"src/docs/components/ai_classifier/#deterministic","title":"\ud83e\udee1 Deterministic\u00b6","text":"<p><code>ai_classifier</code> will be deterministic so long as the underlying model and options does not change.</p>"},{"location":"src/docs/components/ai_function/","title":"AI Function","text":"<p>What it does</p> <p> <code>@ai_fn</code> is a decorator that lets you use LLMs to generate outputs for Python functions without source code.   </p> In\u00a0[\u00a0]: Copied! <pre>from marvin import ai_fn\n\n\n@ai_fn\ndef generate_recipe(ingredients: list[str]) -&gt; list[str]:\n\"\"\"From a list of `ingredients`, generates a\n    complete instruction set to cook a recipe.\n    \"\"\"\n\n\ngenerate_recipe([\"lemon\", \"chicken\", \"olives\", \"coucous\"])\n</pre> from marvin import ai_fn   @ai_fn def generate_recipe(ingredients: list[str]) -&gt; list[str]:     \"\"\"From a list of `ingredients`, generates a     complete instruction set to cook a recipe.     \"\"\"   generate_recipe([\"lemon\", \"chicken\", \"olives\", \"coucous\"]) <p>How it works</p> <p>     AI Functions take your function's name, description, signature, source code, type hints, and provided inputs to predict a likely output. By default, no source code is generated and any existing source code is not executed. The only runtime is the LLM.   </p> <p>When to use</p> <p> <ol> <li> Best for generative tasks: creation and summarization of text or data models.     <li> Best for writing functions that would otherwise be impossible to write.     <li> Great for data extraction, though: see AI Models.     </li></li></li></ol> </p> In\u00a0[4]: Copied! <pre>@ai_fn\ndef list_fruit(n: int, color: str = None) -&gt; list[str]:\n\"\"\"\n    Returns a list of `n` fruit that all have the provided `color`\n    \"\"\"\n</pre> @ai_fn def list_fruit(n: int, color: str = None) -&gt; list[str]:     \"\"\"     Returns a list of `n` fruit that all have the provided `color`     \"\"\" <p>Mapping is invoked by using the AI Function's <code>.map()</code> method. When mapping, you call the function as you normally would, except that each argument should be a list of items. The function will be called on each set of items (e.g. first with each argument's first item, then with each argument's second item, etc.). For example, this is the same as calling <code>list_fruit(2)</code> and <code>list_fruit(3)</code> concurrently:</p> In\u00a0[5]: Copied! <pre>list_fruit.map([2, 3])\n</pre> list_fruit.map([2, 3]) Out[5]: <pre>[['apple', 'banana'], ['apple', 'banana', 'orange']]</pre> <p>And this is the same as calling <code>list_fruit(2, color='orange')</code> and <code>list_fruit(3, color='red')</code> concurrently:</p> In\u00a0[8]: Copied! <pre>list_fruit.map([2, 3], color=[\"orange\", \"red\"])\n</pre> list_fruit.map([2, 3], color=[\"orange\", \"red\"]) Out[8]: <pre>[['orange', 'orange'], ['apple', 'strawberry', 'cherry']]</pre> In\u00a0[\u00a0]: Copied! <pre>from pydantic import BaseModel\nfrom marvin import ai_fn\n\n\nclass SyntheticCustomer(BaseModel):\n    age: int\n    location: str\n    purchase_history: list[str]\n\n\n@ai_fn\ndef generate_synthetic_customer_data(\n    n: int, locations: list[str], average_purchase_history_length: int\n) -&gt; list[SyntheticCustomer]:\n\"\"\"Generates synthetic customer data based on the given parameters.\n    Parameters include the number of customers ('n'),\n    a list of potential locations, and the average length of a purchase history.\n    \"\"\"\n\n\ncustomers = generate_synthetic_customer_data(\n    5, [\"New York\", \"San Francisco\", \"Chicago\"], 3\n)\n</pre> from pydantic import BaseModel from marvin import ai_fn   class SyntheticCustomer(BaseModel):     age: int     location: str     purchase_history: list[str]   @ai_fn def generate_synthetic_customer_data(     n: int, locations: list[str], average_purchase_history_length: int ) -&gt; list[SyntheticCustomer]:     \"\"\"Generates synthetic customer data based on the given parameters.     Parameters include the number of customers ('n'),     a list of potential locations, and the average length of a purchase history.     \"\"\"   customers = generate_synthetic_customer_data(     5, [\"New York\", \"San Francisco\", \"Chicago\"], 3 ) In\u00a0[\u00a0]: Copied! <pre>generate_synthetic_customer_data.prompt(\n    \"I need 10 profiles from rural US cities making between 3 and 7 purchases\"\n)\n</pre> generate_synthetic_customer_data.prompt(     \"I need 10 profiles from rural US cities making between 3 and 7 purchases\" ) <p>\ud83e\uddea Code Generation</p> <p>By default, no code is generated or executed when you call an <code>ai_fn</code>. For those who wish to author code, Marvin exposes an experimental API for code generation. Simply call <code>.code()</code> on an ai_fn, and Marvin will generate the code for you. By default, Marvin will write python code. You can pass a language keyword to generate code in other languages, i.e. <code>.code(language = 'rust')</code>. For best performance give your function a good name, with descriptive docstring, and a signature with type-hints. Provided code will be interpreted as pseudocode.</p> In\u00a0[\u00a0]: hide_cell Copied! <pre>from marvin import ai_fn\n\n\n@ai_fn\ndef fibonacci(n: int) -&gt; int:\n\"\"\"\n    Returns the nth number in the Fibonacci sequence.\n    \"\"\"\n\n\nfibonacci.code(language=\"rust\")\n</pre> from marvin import ai_fn   @ai_fn def fibonacci(n: int) -&gt; int:     \"\"\"     Returns the nth number in the Fibonacci sequence.     \"\"\"   fibonacci.code(language=\"rust\") In\u00a0[\u00a0]: Copied! <pre>@ai_fn\ndef analyze_customer_sentiment(reviews: list[str]) -&gt; dict:\n\"\"\"\n    Returns an analysis of customer sentiment, including common\n    complaints, praises, and suggestions, from a list of product\n    reviews.\n    \"\"\"\n\n\n# analyze_customer_sentiment([\"I love this product!\", \"I hate this product!\"])\n</pre> @ai_fn def analyze_customer_sentiment(reviews: list[str]) -&gt; dict:     \"\"\"     Returns an analysis of customer sentiment, including common     complaints, praises, and suggestions, from a list of product     reviews.     \"\"\"   # analyze_customer_sentiment([\"I love this product!\", \"I hate this product!\"]) In\u00a0[\u00a0]: Copied! <pre>class FinancialReport(pydantic.BaseModel):\n    ...\n\n\n@ai_fn\ndef create_drip_email(n: int, market_conditions: str) -&gt; list[FinancialReport]:\n\"\"\"\n    Generates `n` synthetic financial reports based on specified\n    `market_conditions` (e.g., 'recession', 'bull market', 'stagnant economy').\n    \"\"\"\n</pre> class FinancialReport(pydantic.BaseModel):     ...   @ai_fn def create_drip_email(n: int, market_conditions: str) -&gt; list[FinancialReport]:     \"\"\"     Generates `n` synthetic financial reports based on specified     `market_conditions` (e.g., 'recession', 'bull market', 'stagnant economy').     \"\"\" In\u00a0[\u00a0]: Copied! <pre>class IoTData(pydantic.BaseModel):\n    ...\n\n\n@ai_fn\ndef generate_synthetic_IoT_data(n: int, device_type: str) -&gt; list[IoTData]:\n\"\"\"\n    Generates `n` synthetic data points mimicking those from a specified\n    `device_type` in an IoT system.\n    \"\"\"\n</pre> class IoTData(pydantic.BaseModel):     ...   @ai_fn def generate_synthetic_IoT_data(n: int, device_type: str) -&gt; list[IoTData]:     \"\"\"     Generates `n` synthetic data points mimicking those from a specified     `device_type` in an IoT system.     \"\"\""},{"location":"src/docs/components/ai_function/#ai-function","title":"AI Function\u00b6","text":"<p>AI Functions are a high-level component, or building block, of Marvin. Like all Marvin components, they are completely standalone: you're free to use them with or without the rest of Marvin.</p>"},{"location":"src/docs/components/ai_function/#mapping","title":"Mapping\u00b6","text":"<p>AI Functions can be mapped over sequences of arguments. Mapped functions run concurrently, which means they run practically in parallel (since they are IO-bound). Therefore, the map will complete as soon as the slowest function call finishes.</p> <p>To see how mapping works, consider this AI Function:</p>"},{"location":"src/docs/components/ai_function/#features","title":"Features\u00b6","text":""},{"location":"src/docs/components/ai_function/#type-safe","title":"\u2699\ufe0f Type Safe\u00b6","text":"<p><code>ai_fn</code> is fully type-safe. It works out of the box with Pydantic models in your function's parameters or return type.</p>"},{"location":"src/docs/components/ai_function/#natural-language-api","title":"\ud83d\udde3\ufe0f Natural Language API\u00b6","text":"<p>Marvin exposes an API to prompt an <code>ai_fn</code> with natural language. This lets you create a Language API for any function you can write down.</p>"},{"location":"src/docs/components/ai_function/#examples","title":"Examples\u00b6","text":""},{"location":"src/docs/components/ai_function/#customer-sentiment","title":"Customer Sentiment\u00b6","text":"<p>Rapidly prototype natural language pipelines.</p> <p>     Use hallucination as a literal feature. Generate data that would be impossible     or prohibatively expensive to purchase as you rapidly protype NLP pipelines.    </p>"},{"location":"src/docs/components/ai_function/#generate-synthetic-data","title":"Generate Synthetic Data\u00b6","text":"<p>General real fake data.</p> <p>     Use hallucination as a figurative feature. Use python or pydantic     to describe the data model you need, and generate realistic data on the fly      for sales demos.   </p>"},{"location":"src/docs/components/ai_model/","title":"AI Model","text":"<p>What it does</p> <p>     A decorator that lets you extract structured data from unstructured text, documents, or instructions.   </p> In\u00a0[\u00a0]: Copied! <pre>from marvin import ai_model\nfrom pydantic import BaseModel, Field\n\n\n@ai_model\nclass Location(BaseModel):\n    city: str\n    state: str = Field(..., description=\"The two-letter state abbreviation\")\n\n\nLocation(\"The Big Apple\")\n# Location(city='New York', state='NY')\n</pre> from marvin import ai_model from pydantic import BaseModel, Field   @ai_model class Location(BaseModel):     city: str     state: str = Field(..., description=\"The two-letter state abbreviation\")   Location(\"The Big Apple\") # Location(city='New York', state='NY') <p>How it works</p> <p>     AI Models use an LLM to extract, infer, or deduce data from the provided text. The data is parsed with Pydantic into the provided schema.   </p> <p>When to use</p> <p> <ol> <li> Best for extractive tasks: structuring of text or data models.     <li> Best for writing NLP pipelines that would otherwise be impossible to create.     <li> Good for model generation, though, see AI Function.     </li></li></li></ol> </p> In\u00a0[\u00a0]: Copied! <pre>@ai_model\nclass Location(BaseModel):\n    city: str\n    state: str\n    country: str\n    latitude: float\n    longitude: float\n\n\nLocation(\"He says he's from the windy city\")\n\n# Location(\n#   city='Chicago',\n#   state='Illinois',\n#   country='United States',\n#   latitude=41.8781,\n#   longitude=-87.6298\n# )\n</pre> @ai_model class Location(BaseModel):     city: str     state: str     country: str     latitude: float     longitude: float   Location(\"He says he's from the windy city\")  # Location( #   city='Chicago', #   state='Illinois', #   country='United States', #   latitude=41.8781, #   longitude=-87.6298 # ) In\u00a0[\u00a0]: Copied! <pre>from typing import Optional\nfrom pydantic import BaseModel\nfrom marvin import ai_model\n\n\n@ai_model\nclass Resume(BaseModel):\n    first_name: str\n    last_name: str\n    phone_number: Optional[str]\n    email: str\n\n\nResume(\"Ford Prefect \u2022 (555) 5124-5242 \u2022 ford@prefect.io\").json(indent=2)\n\n# {\n# first_name: 'Ford',\n# last_name: 'Prefect',\n# email: 'ford@prefect.io',\n# phone: '(555) 5124-5242',\n# }\n</pre> from typing import Optional from pydantic import BaseModel from marvin import ai_model   @ai_model class Resume(BaseModel):     first_name: str     last_name: str     phone_number: Optional[str]     email: str   Resume(\"Ford Prefect \u2022 (555) 5124-5242 \u2022 ford@prefect.io\").json(indent=2)  # { # first_name: 'Ford', # last_name: 'Prefect', # email: 'ford@prefect.io', # phone: '(555) 5124-5242', # } In\u00a0[\u00a0]: Copied! <pre>import datetime\nfrom typing import Optional, List\nfrom pydantic import BaseModel\nfrom marvin import ai_model\n\n\nclass Destination(pydantic.BaseModel):\n    start: datetime.date\n    end: datetime.date\n    city: Optional[str]\n    country: str\n    suggested_attractions: list[str]\n\n\n@ai_model\nclass Trip(pydantic.BaseModel):\n    trip_start: datetime.date\n    trip_end: datetime.date\n    trip_preferences: list[str]\n    destinations: List[Destination]\n\n\nTrip(\"\"\"\\\n    I've got all of June off, so hoping to spend the first\\\n    half of June in London and the second half in Rabat. I love \\\n    good food and going to museums.\n\"\"\").json(indent=2)\n\n# {\n#   \"trip_start\": \"2023-06-01\",\n#   \"trip_end\": \"2023-06-30\",\n#   \"trip_preferences\": [\n#     \"good food\",\n#     \"museums\"\n#   ],\n#   \"destinations\": [\n#     {\n#       \"start\": \"2023-06-01\",\n#       \"end\": \"2023-06-15\",\n#       \"city\": \"London\",\n#       \"country\": \"United Kingdom\",\n#       \"suggested_attractions\": [\n#         \"British Museum\",\n#         \"Tower of London\",\n#         \"Borough Market\"\n#       ]\n#     },\n#     {\n#       \"start\": \"2023-06-16\",\n#       \"end\": \"2023-06-30\",\n#       \"city\": \"Rabat\",\n#       \"country\": \"Morocco\",\n#       \"suggested_attractions\": [\n#         \"Kasbah des Oudaias\",\n#         \"Hassan Tower\",\n#         \"Rabat Archaeological Museum\"\n#       ]\n#     }\n#   ]\n# }\n</pre> import datetime from typing import Optional, List from pydantic import BaseModel from marvin import ai_model   class Destination(pydantic.BaseModel):     start: datetime.date     end: datetime.date     city: Optional[str]     country: str     suggested_attractions: list[str]   @ai_model class Trip(pydantic.BaseModel):     trip_start: datetime.date     trip_end: datetime.date     trip_preferences: list[str]     destinations: List[Destination]   Trip(\"\"\"\\     I've got all of June off, so hoping to spend the first\\     half of June in London and the second half in Rabat. I love \\     good food and going to museums. \"\"\").json(indent=2)  # { #   \"trip_start\": \"2023-06-01\", #   \"trip_end\": \"2023-06-30\", #   \"trip_preferences\": [ #     \"good food\", #     \"museums\" #   ], #   \"destinations\": [ #     { #       \"start\": \"2023-06-01\", #       \"end\": \"2023-06-15\", #       \"city\": \"London\", #       \"country\": \"United Kingdom\", #       \"suggested_attractions\": [ #         \"British Museum\", #         \"Tower of London\", #         \"Borough Market\" #       ] #     }, #     { #       \"start\": \"2023-06-16\", #       \"end\": \"2023-06-30\", #       \"city\": \"Rabat\", #       \"country\": \"Morocco\", #       \"suggested_attractions\": [ #         \"Kasbah des Oudaias\", #         \"Hassan Tower\", #         \"Rabat Archaeological Museum\" #       ] #     } #   ] # } In\u00a0[\u00a0]: Copied! <pre>from datetime import date\nfrom typing import Optional, List\nfrom pydantic import BaseModel\n\n\nclass Patient(BaseModel):\n    name: str\n    age: int\n    is_smoker: bool\n\n\nclass Diagnosis(BaseModel):\n    condition: str\n    diagnosis_date: date\n    stage: Optional[str] = None\n    type: Optional[str] = None\n    histology: Optional[str] = None\n    complications: Optional[str] = None\n\n\nclass Treatment(BaseModel):\n    name: str\n    start_date: date\n    end_date: Optional[date] = None\n\n\nclass Medication(Treatment):\n    dose: Optional[str] = None\n\n\nclass BloodTest(BaseModel):\n    name: str\n    result: str\n    test_date: date\n\n\n@ai_model\nclass PatientData(BaseModel):\n    patient: Patient\n    diagnoses: List[Diagnosis]\n    treatments: List[Treatment]\n    blood_tests: List[BloodTest]\n\n\nPatientData(\"\"\"\\\nMs. Lee, a 45-year-old patient, was diagnosed with type 2 diabetes mellitus on 06-01-2018.\nUnfortunately, Ms. Lee's diabetes has progressed and she developed diabetic retinopathy on 09-01-2019.\nMs. Lee was diagnosed with type 2 diabetes mellitus on 06-01-2018.\nMs. Lee was initially diagnosed with stage I hypertension on 06-01-2018.\nMs. Lee's blood work revealed hyperlipidemia with elevated LDL levels on 06-01-2018.\nMs. Lee was prescribed metformin 1000 mg daily for her diabetes on 06-01-2018.\nMs. Lee's most recent A1C level was 8.5% on 06-15-2020.\nMs. Lee was diagnosed with type 2 diabetes mellitus, with microvascular complications, including diabetic retinopathy, on 09-01-2019.\nMs. Lee's blood pressure remains elevated and she was prescribed lisinopril 10 mg daily on 09-01-2019.\nMs. Lee's most recent lipid panel showed elevated LDL levels, and she was prescribed atorvastatin 40 mg daily on 09-01-2019.\\\n\"\"\").json(indent=2)\n\n# {\n#   \"patient\": {\n#     \"name\": \"Ms. Lee\",\n#     \"age\": 45,\n#     \"is_smoker\": false\n#   },\n#   \"diagnoses\": [\n#     {\n#       \"condition\": \"Type 2 diabetes mellitus\",\n#       \"diagnosis_date\": \"2018-06-01\",\n#       \"stage\": \"I\",\n#       \"type\": null,\n#       \"histology\": null,\n#       \"complications\": null\n#     },\n#     {\n#       \"condition\": \"Diabetic retinopathy\",\n#       \"diagnosis_date\": \"2019-09-01\",\n#       \"stage\": null,\n#       \"type\": null,\n#       \"histology\": null,\n#       \"complications\": null\n#     }\n#   ],\n#   \"treatments\": [\n#     {\n#       \"name\": \"Metformin\",\n#       \"start_date\": \"2018-06-01\",\n#       \"end_date\": null\n#     },\n#     {\n#       \"name\": \"Lisinopril\",\n#       \"start_date\": \"2019-09-01\",\n#       \"end_date\": null\n#     },\n#     {\n#       \"name\": \"Atorvastatin\",\n#       \"start_date\": \"2019-09-01\",\n#       \"end_date\": null\n#     }\n#   ],\n#   \"blood_tests\": [\n#     {\n#       \"name\": \"A1C\",\n#       \"result\": \"8.5%\",\n#       \"test_date\": \"2020-06-15\"\n#     },\n#     {\n#       \"name\": \"LDL\",\n#       \"result\": \"Elevated\",\n#       \"test_date\": \"2018-06-01\"\n#     },\n#     {\n#       \"name\": \"LDL\",\n#       \"result\": \"Elevated\",\n#       \"test_date\": \"2019-09-01\"\n#     }\n#   ]\n# }\n</pre> from datetime import date from typing import Optional, List from pydantic import BaseModel   class Patient(BaseModel):     name: str     age: int     is_smoker: bool   class Diagnosis(BaseModel):     condition: str     diagnosis_date: date     stage: Optional[str] = None     type: Optional[str] = None     histology: Optional[str] = None     complications: Optional[str] = None   class Treatment(BaseModel):     name: str     start_date: date     end_date: Optional[date] = None   class Medication(Treatment):     dose: Optional[str] = None   class BloodTest(BaseModel):     name: str     result: str     test_date: date   @ai_model class PatientData(BaseModel):     patient: Patient     diagnoses: List[Diagnosis]     treatments: List[Treatment]     blood_tests: List[BloodTest]   PatientData(\"\"\"\\ Ms. Lee, a 45-year-old patient, was diagnosed with type 2 diabetes mellitus on 06-01-2018. Unfortunately, Ms. Lee's diabetes has progressed and she developed diabetic retinopathy on 09-01-2019. Ms. Lee was diagnosed with type 2 diabetes mellitus on 06-01-2018. Ms. Lee was initially diagnosed with stage I hypertension on 06-01-2018. Ms. Lee's blood work revealed hyperlipidemia with elevated LDL levels on 06-01-2018. Ms. Lee was prescribed metformin 1000 mg daily for her diabetes on 06-01-2018. Ms. Lee's most recent A1C level was 8.5% on 06-15-2020. Ms. Lee was diagnosed with type 2 diabetes mellitus, with microvascular complications, including diabetic retinopathy, on 09-01-2019. Ms. Lee's blood pressure remains elevated and she was prescribed lisinopril 10 mg daily on 09-01-2019. Ms. Lee's most recent lipid panel showed elevated LDL levels, and she was prescribed atorvastatin 40 mg daily on 09-01-2019.\\ \"\"\").json(indent=2)  # { #   \"patient\": { #     \"name\": \"Ms. Lee\", #     \"age\": 45, #     \"is_smoker\": false #   }, #   \"diagnoses\": [ #     { #       \"condition\": \"Type 2 diabetes mellitus\", #       \"diagnosis_date\": \"2018-06-01\", #       \"stage\": \"I\", #       \"type\": null, #       \"histology\": null, #       \"complications\": null #     }, #     { #       \"condition\": \"Diabetic retinopathy\", #       \"diagnosis_date\": \"2019-09-01\", #       \"stage\": null, #       \"type\": null, #       \"histology\": null, #       \"complications\": null #     } #   ], #   \"treatments\": [ #     { #       \"name\": \"Metformin\", #       \"start_date\": \"2018-06-01\", #       \"end_date\": null #     }, #     { #       \"name\": \"Lisinopril\", #       \"start_date\": \"2019-09-01\", #       \"end_date\": null #     }, #     { #       \"name\": \"Atorvastatin\", #       \"start_date\": \"2019-09-01\", #       \"end_date\": null #     } #   ], #   \"blood_tests\": [ #     { #       \"name\": \"A1C\", #       \"result\": \"8.5%\", #       \"test_date\": \"2020-06-15\" #     }, #     { #       \"name\": \"LDL\", #       \"result\": \"Elevated\", #       \"test_date\": \"2018-06-01\" #     }, #     { #       \"name\": \"LDL\", #       \"result\": \"Elevated\", #       \"test_date\": \"2019-09-01\" #     } #   ] # } In\u00a0[\u00a0]: Copied! <pre>from datetime import date\nfrom typing import Optional\nfrom pydantic import BaseModel\nfrom django.db.models import Q\n\n\nclass DjangoLookup(BaseModel):\n    field: Literal[*django_fields]\n    lookup: Literal[*django_lookups] = pydantic.Field(description=\"e.g. __iregex\")\n    value: Any\n\n\n@ai_model\nclass DjangoQuery(BaseModel):\n\"\"\"A model representing a Django ORM query\"\"\"\n\n    lookups: List[DjangoLookup]\n\n    def to_q(self) -&gt; Q:\n        q = Q()\n        for lookup in self.lookups:\n            q &amp;= Q(**{f\"{lookup.field}__{lookup.lookup}\": lookup.value})\n        return q\n\n\nDjangoQuery(\"\"\"\\\n    All users who joined more than two months ago but\\\n    haven't made a purchase in the last 30 days\"\"\").to_q()\n\n# &lt;Q: (AND:\n#     ('date_joined__lte', '2023-03-11'),\n#     ('last_purchase_date__isnull', False),\n#     ('last_purchase_date__lte', '2023-04-11'))&gt;\n</pre> from datetime import date from typing import Optional from pydantic import BaseModel from django.db.models import Q   class DjangoLookup(BaseModel):     field: Literal[*django_fields]     lookup: Literal[*django_lookups] = pydantic.Field(description=\"e.g. __iregex\")     value: Any   @ai_model class DjangoQuery(BaseModel):     \"\"\"A model representing a Django ORM query\"\"\"      lookups: List[DjangoLookup]      def to_q(self) -&gt; Q:         q = Q()         for lookup in self.lookups:             q &amp;= Q(**{f\"{lookup.field}__{lookup.lookup}\": lookup.value})         return q   DjangoQuery(\"\"\"\\     All users who joined more than two months ago but\\     haven't made a purchase in the last 30 days\"\"\").to_q()  #  In\u00a0[\u00a0]: Copied! <pre>from datetime import date\nfrom typing import Optional\nfrom pydantic import BaseModel\n\n\n@ai_model\nclass CapTable(BaseModel):\n    total_authorized_shares: int\n    total_common_share: int\n    total_common_shares_outstanding: Optional[int]\n    total_preferred_shares: int\n    conversion_price_multiple: int = 1\n\n\nCapTable(\"\"\"\\\n    In the cap table for Charter, the total authorized shares amount to 13,250,000. \n    The total number of common shares stands at 10,000,000 as specified in Article Fourth, \n    clause (i) and Section 2.2(a)(i). The exact count of common shares outstanding is not \n    available at the moment. Furthermore, there are a total of 3,250,000 preferred shares mentioned \n    in Article Fourth, clause (ii) and Section 2.2(a)(ii). The dividend percentage for Charter is \n    set at 8.00%. Additionally, the mandatory conversion price multiple is 3x, which is \n    derived from the Term Sheet.\\\n\"\"\").json(indent=2)\n\n# {\n#   \"total_authorized_shares\": 13250000,\n#   \"total_common_share\": 10000000,\n#   \"total_common_shares_outstanding\": null,\n#   \"total_preferred_shares\": 3250000,\n#   \"conversion_price_multiple\": 3\n# }\n</pre> from datetime import date from typing import Optional from pydantic import BaseModel   @ai_model class CapTable(BaseModel):     total_authorized_shares: int     total_common_share: int     total_common_shares_outstanding: Optional[int]     total_preferred_shares: int     conversion_price_multiple: int = 1   CapTable(\"\"\"\\     In the cap table for Charter, the total authorized shares amount to 13,250,000.      The total number of common shares stands at 10,000,000 as specified in Article Fourth,      clause (i) and Section 2.2(a)(i). The exact count of common shares outstanding is not      available at the moment. Furthermore, there are a total of 3,250,000 preferred shares mentioned      in Article Fourth, clause (ii) and Section 2.2(a)(ii). The dividend percentage for Charter is      set at 8.00%. Additionally, the mandatory conversion price multiple is 3x, which is      derived from the Term Sheet.\\ \"\"\").json(indent=2)  # { #   \"total_authorized_shares\": 13250000, #   \"total_common_share\": 10000000, #   \"total_common_shares_outstanding\": null, #   \"total_preferred_shares\": 3250000, #   \"conversion_price_multiple\": 3 # } In\u00a0[\u00a0]: Copied! <pre>import datetime\nfrom typing import List\nfrom pydantic import BaseModel\nfrom typing_extensions import Literal\nfrom marvin import ai_model\n\n\nclass ActionItem(BaseModel):\n    responsible: str\n    description: str\n    deadline: Optional[datetime.datetime]\n    time_sensitivity: Literal[\"low\", \"medium\", \"high\"]\n\n\n@ai_model\nclass Conversation(BaseModel):\n\"\"\"A class representing a team conversation\"\"\"\n\n    participants: List[str]\n    action_items: List[ActionItem]\n\n\nConversation(\"\"\"\n    Adam: Hey Jeremiah can you approve my PR? I requested you to review it.\n    Jeremiah: Yeah sure, when do you need it done by?\n    Adam: By this Friday at the latest, we need to ship it by end of week.\n    Jeremiah: Oh shoot, I need to make sure that Nate and I have a chance to chat first.\n    Nate: Jeremiah we can meet today to chat.\n    Jeremiah: Okay, I'll book something for today.\n\"\"\").json(indent=2)\n\n# {\n#   \"participants\": [\n#     \"Adam\",\n#     \"Jeremiah\",\n#     \"Nate\"\n#   ],\n#   \"action_items\": [\n#     {\n#       \"responsible\": \"Jeremiah\",\n#       \"description\": \"Approve Adam's PR\",\n#       \"deadline\": \"2023-05-12T23:59:59\",\n#       \"time_sensitivity\": \"high\"\n#     },\n#     {\n#       \"responsible\": \"Jeremiah\",\n#       \"description\": \"Book a meeting with Nate\",\n#       \"deadline\": \"2023-05-11T23:59:59\",\n#       \"time_sensitivity\": \"high\"\n#     }\n#   ]\n# }\n</pre> import datetime from typing import List from pydantic import BaseModel from typing_extensions import Literal from marvin import ai_model   class ActionItem(BaseModel):     responsible: str     description: str     deadline: Optional[datetime.datetime]     time_sensitivity: Literal[\"low\", \"medium\", \"high\"]   @ai_model class Conversation(BaseModel):     \"\"\"A class representing a team conversation\"\"\"      participants: List[str]     action_items: List[ActionItem]   Conversation(\"\"\"     Adam: Hey Jeremiah can you approve my PR? I requested you to review it.     Jeremiah: Yeah sure, when do you need it done by?     Adam: By this Friday at the latest, we need to ship it by end of week.     Jeremiah: Oh shoot, I need to make sure that Nate and I have a chance to chat first.     Nate: Jeremiah we can meet today to chat.     Jeremiah: Okay, I'll book something for today. \"\"\").json(indent=2)  # { #   \"participants\": [ #     \"Adam\", #     \"Jeremiah\", #     \"Nate\" #   ], #   \"action_items\": [ #     { #       \"responsible\": \"Jeremiah\", #       \"description\": \"Approve Adam's PR\", #       \"deadline\": \"2023-05-12T23:59:59\", #       \"time_sensitivity\": \"high\" #     }, #     { #       \"responsible\": \"Jeremiah\", #       \"description\": \"Book a meeting with Nate\", #       \"deadline\": \"2023-05-11T23:59:59\", #       \"time_sensitivity\": \"high\" #     } #   ] # }"},{"location":"src/docs/components/ai_model/#ai-model","title":"AI Model\u00b6","text":"<p>AI Models are a high-level component, or building block, of Marvin. Like all Marvin components, they are completely standalone: you're free to use them with or without the rest of Marvin.</p>"},{"location":"src/docs/components/ai_model/#features","title":"Features\u00b6","text":""},{"location":"src/docs/components/ai_model/#type-safe","title":"\u2699\ufe0f Type Safe\u00b6","text":"<p><code>ai_model</code> is fully type-safe. It works out of the box with Pydantic models.</p>"},{"location":"src/docs/components/ai_model/#powered-by-deduction","title":"\ud83e\udde0 Powered by deduction\u00b6","text":"<p><code>ai_model</code> gives your data model access to the knowledge and deductive power of a Large Language Model. This means that your data model can infer answers to previous impossible tasks.</p>"},{"location":"src/docs/components/ai_model/#examples","title":"Examples\u00b6","text":""},{"location":"src/docs/components/ai_model/#resumes","title":"Resumes\u00b6","text":""},{"location":"src/docs/components/ai_model/#customer-service","title":"Customer Service\u00b6","text":""},{"location":"src/docs/components/ai_model/#electronic-health-records","title":"Electronic Health Records\u00b6","text":""},{"location":"src/docs/components/ai_model/#text-to-sql","title":"Text to SQL\u00b6","text":""},{"location":"src/docs/components/ai_model/#financial-reports","title":"Financial Reports\u00b6","text":""},{"location":"src/docs/components/ai_model/#meeting-notes","title":"Meeting Notes\u00b6","text":""},{"location":"src/docs/components/ai_model_factory/","title":"Model Factories","text":""},{"location":"src/docs/components/ai_model_factory/#ai_fn","title":"<code>ai_fn</code>","text":"<p><code>ai_fn</code> enables you to use Large Language Models to evaluate python functions. </p>"},{"location":"src/docs/components/overview/","title":"Overview","text":"In\u00a0[2]: Copied! <pre>from marvin import ai_model\nfrom pydantic import BaseModel, Field\n\n\n@ai_model\nclass Location(BaseModel):\n    city: str\n    state: str = Field(..., description=\"The two-letter state abbreviation\")\n\n\nLocation(\"The Big Apple\")\n</pre> from marvin import ai_model from pydantic import BaseModel, Field   @ai_model class Location(BaseModel):     city: str     state: str = Field(..., description=\"The two-letter state abbreviation\")   Location(\"The Big Apple\") Out[2]: <pre>Location(city='New York', state='NY')</pre> In\u00a0[13]: Copied! <pre>from marvin import ai_classifier\nfrom enum import Enum\n\n\n@ai_classifier\nclass AppRoute(Enum):\n\"\"\"Represents distinct routes command bar for a different application\"\"\"\n\n    USER_PROFILE = \"/user-profile\"\n    SEARCH = \"/search\"\n    NOTIFICATIONS = \"/notifications\"\n    SETTINGS = \"/settings\"\n    HELP = \"/help\"\n    CHAT = \"/chat\"\n    DOCS = \"/docs\"\n    PROJECTS = \"/projects\"\n    WORKSPACES = \"/workspaces\"\n\n\nAppRoute(\"update my name\")\n</pre> from marvin import ai_classifier from enum import Enum   @ai_classifier class AppRoute(Enum):     \"\"\"Represents distinct routes command bar for a different application\"\"\"      USER_PROFILE = \"/user-profile\"     SEARCH = \"/search\"     NOTIFICATIONS = \"/notifications\"     SETTINGS = \"/settings\"     HELP = \"/help\"     CHAT = \"/chat\"     DOCS = \"/docs\"     PROJECTS = \"/projects\"     WORKSPACES = \"/workspaces\"   AppRoute(\"update my name\") Out[13]: <pre>&lt;AppRoute.USER_PROFILE: '/user-profile'&gt;</pre> In\u00a0[9]: Copied! <pre>from marvin import ai_fn\n\n\n@ai_fn\ndef sentiment(text: str) -&gt; float:\n\"\"\"\n    Given `text`, returns a number between 1 (positive) and -1 (negative)\n    indicating its sentiment score.\n    \"\"\"\n\n\nprint(\"Text 1:\", sentiment(\"I love working with Marvin!\"))\nprint(\"Text 2:\", sentiment(\"These examples could use some work...\"))\n</pre> from marvin import ai_fn   @ai_fn def sentiment(text: str) -&gt; float:     \"\"\"     Given `text`, returns a number between 1 (positive) and -1 (negative)     indicating its sentiment score.     \"\"\"   print(\"Text 1:\", sentiment(\"I love working with Marvin!\")) print(\"Text 2:\", sentiment(\"These examples could use some work...\")) <pre>Text 1: 0.8\nText 2: -0.2\n</pre> <p>Because AI functions are just like regular functions, you can quickly modify them for your needs. Here, we modify the above example to work with multiple strings at once:</p> In\u00a0[11]: Copied! <pre>from marvin import ai_fn\n\n\n@ai_fn\ndef sentiment_list(texts: list[str]) -&gt; list[float]:\n\"\"\"\n    Given a list of `texts`, returns a list of numbers between 1 (positive) and\n    -1 (negative) indicating their respective sentiment scores.\n    \"\"\"\n\n\nsentiment_list(\n    [\n        \"That was surprisingly easy!\",\n        \"Oh no, not again.\",\n    ]\n)\n</pre> from marvin import ai_fn   @ai_fn def sentiment_list(texts: list[str]) -&gt; list[float]:     \"\"\"     Given a list of `texts`, returns a list of numbers between 1 (positive) and     -1 (negative) indicating their respective sentiment scores.     \"\"\"   sentiment_list(     [         \"That was surprisingly easy!\",         \"Oh no, not again.\",     ] ) Out[11]: <pre>[0.7, -0.5]</pre> In\u00a0[10]: Copied! <pre>from datetime import datetime\nfrom pydantic import BaseModel, Field\nfrom marvin import AIApplication\n\n\n# create models to represent the state of our ToDo app\nclass ToDo(BaseModel):\n    title: str\n    description: str = None\n    due_date: datetime = None\n    done: bool = False\n\n\nclass ToDoState(BaseModel):\n    todos: list[ToDo] = []\n\n\n# create the app with an initial state and description\ntodo_app = AIApplication(\n    state=ToDoState(),\n    description=(\n        \"A simple todo app. Users will provide instructions for creating and updating\"\n        \" their todo lists.\"\n    ),\n)\n</pre> from datetime import datetime from pydantic import BaseModel, Field from marvin import AIApplication   # create models to represent the state of our ToDo app class ToDo(BaseModel):     title: str     description: str = None     due_date: datetime = None     done: bool = False   class ToDoState(BaseModel):     todos: list[ToDo] = []   # create the app with an initial state and description todo_app = AIApplication(     state=ToDoState(),     description=(         \"A simple todo app. Users will provide instructions for creating and updating\"         \" their todo lists.\"     ), ) <p>Now we can invoke the app directly to add a to-do item. Note that the app understands that it is supposed to manipulate state, not just respond conversationally.</p> In\u00a0[11]: Copied! <pre># invoke the application by adding a todo\nresponse = todo_app(\"I need to go to the store tomorrow at 5pm\")\n\n\nprint(f\"Response: {response.content}\\n\")\nprint(f\"App state: {todo_app.state.json(indent=2)}\")\n</pre> # invoke the application by adding a todo response = todo_app(\"I need to go to the store tomorrow at 5pm\")   print(f\"Response: {response.content}\\n\") print(f\"App state: {todo_app.state.json(indent=2)}\") <pre>Response: Got it! I've added a new task to your to-do list. You need to go to the store tomorrow at 5pm.\n\nApp state: {\n  \"todos\": [\n    {\n      \"title\": \"Go to the store\",\n      \"description\": \"Buy groceries\",\n      \"due_date\": \"2023-07-12T17:00:00+00:00\",\n      \"done\": false\n    }\n  ]\n}\n</pre> <p>We can inform the app that we already finished the task, and it updates state appropriately</p> In\u00a0[12]: Copied! <pre># complete the task\nresponse = todo_app(\"I already went\")\n\n\nprint(f\"Response: {response.content}\\n\")\nprint(f\"App state: {todo_app.state.json(indent=2)}\")\n</pre> # complete the task response = todo_app(\"I already went\")   print(f\"Response: {response.content}\\n\") print(f\"App state: {todo_app.state.json(indent=2)}\") <pre>Response: Great! I've marked the task as completed. Is there anything else you'd like to add to your to-do list?\n\nApp state: {\n  \"todos\": [\n    {\n      \"title\": \"Go to the store\",\n      \"description\": \"Buy groceries\",\n      \"due_date\": \"2023-07-12T17:00:00+00:00\",\n      \"done\": true\n    }\n  ]\n}\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"src/docs/components/overview/#ai-components","title":"AI Components\u00b6","text":"<p>Marvin introduces a number of components that can become the building blocks of AI-powered software.</p>"},{"location":"src/docs/components/overview/#ai-models","title":"AI Models\u00b6","text":"<p>Marvin's most basic component is the AI Model, a drop-in replacement for Pydantic's <code>BaseModel</code>. AI Models can be instantiated from any string, making them ideal for structuring data, entity extraction, and synthetic data generation:</p>"},{"location":"src/docs/components/overview/#ai-classifiers","title":"AI Classifiers\u00b6","text":"<p>AI Classifiers let you build multi-label classifiers with no code and no training data. It enumerates your options, and uses a clever logit bias trick to force an LLM to deductively choose the index of the best option given your provided input. It then returns the choice associated to that index. It's bulletproof, cost-effective, and lets you build classifiers as quickly as you can write your classes.</p>"},{"location":"src/docs/components/overview/#ai-functions","title":"AI Functions\u00b6","text":"<p>AI Functions look like regular functions, but have no source code. Instead, an AI uses their description and inputs to generate their outputs, making them ideal for NLP applications like sentiment analysis.</p>"},{"location":"src/docs/components/overview/#ai-applications","title":"AI Applications\u00b6","text":"<p>AI Applications are the base class for interactive use cases. They are designed to be invoked one or more times, and automatically maintain three forms of state:</p> <ul> <li><code>state</code>: a structured application state</li> <li><code>plan</code>: high-level planning for the AI assistant to keep the application \"on-track\" across multiple invocations</li> <li><code>history</code>: a history of all LLM interactions</li> </ul> <p>AI Applications can be used to implement many \"classic\" LLM use cases, such as chatbots, tool-using agents, developer assistants, and more. In addition, thanks to their persistent state and planning, they can implement applications that don't have a traditional chat UX, such as a ToDo app. Here's an example:</p>"},{"location":"src/docs/configuration/anthropic/","title":"Anthropic","text":"<p>Available in Marvin 1.1</p> <p>Marvin supports Anthropic's Claude 1 and Claude 2 models. In order to use the Anthropic API, you must provide an API key.</p> <p>Installing the Anthropic provider</p> <p>To use the Anthropic provider, you must have the <code>anthropic</code> Python client installed. You can do this by installing Marvin as <code>pip install \"marvin[anthropic]\"</code></p> <p>Anthropic is not optimized for calling functions</p> <p>Anthropic's models are not fine-tuned for calling functions or generating structured outputs. Therefore, Marvin adds a significant number of additional instructions to get Anthropic models to mimic this behavior. Empirically, this works very well for most Marvin components, including functions, models, and classifiers. However, it may not perform as well for more complex AI Applications.</p>"},{"location":"src/docs/configuration/anthropic/#getting-an-api-key","title":"Getting an API key","text":"<p>To obtain an Anthropic API key, follow these steps:</p> <ol> <li>Log in to your Anthropic account (sign up if you don't have one)</li> <li>Go to the \"API Keys\" page under your account settings.</li> <li>Click \"Create Key.\" A new API key will be generated. Make sure to copy the key to your clipboard, as you will not be able to see it again.</li> </ol>"},{"location":"src/docs/configuration/anthropic/#setting-the-api-key","title":"Setting the API key","text":"<p>You can set your API key at runtime like this:</p> <pre><code>import marvin\nmarvin.settings.anthropic.api_key = YOUR_API_KEY\n</code></pre> <p>However, it is preferable to pass sensitive settings as an environment variable: <code>MARVIN_ANTHROPIC_API_KEY</code>.</p> <p>To set your Anthropic API key as an environment variable, open your terminal and run the following command, replacing  with the actual key: <pre><code>export MARVIN_ANTHROPIC_API_KEY=&lt;your API key&gt;\n</code></pre> <p>This will set the key for the duration of your terminal session. To set it more permanently, configure your terminal or its respective env files.</p>"},{"location":"src/docs/configuration/anthropic/#using-a-model","title":"Using a model","text":"<p>Once your API key is set, you can use any valid Anthropic model by providing it as Marvin's <code>llm_model</code> setting: <pre><code>import marvin\nmarvin.settings.llm_model = 'claude-2'\n</code></pre></p> <p>Marvin will automatically recognize that the <code>claude-*</code> family of models use the Anthropic provider. To indicate a provider explicitly, prefix the model name with <code>anthropic/</code>. For example: <code>marvin.settings.llm_model = 'anthropic/claude-2'</code>.</p>"},{"location":"src/docs/configuration/openai/","title":"OpenAI","text":"<p>Marvin supports OpenAI's GPT-3.5 and GPT-4 models, and uses the <code>gpt-3.5-turbo</code> model by default. In order to use the OpenAI API, you must provide an API key.</p>"},{"location":"src/docs/configuration/openai/#getting-an-api-key","title":"Getting an API key","text":"<p>To obtain an OpenAI API key, follow these steps:</p> <ol> <li>Log in to your OpenAI account (sign up if you don't have one)</li> <li>Go to the \"API Keys\" page under your account settings.</li> <li>Click \"Create new secret key.\" A new API key will be generated. Make sure to copy the key to your clipboard, as you will not be able to see it again.</li> </ol>"},{"location":"src/docs/configuration/openai/#setting-the-api-key","title":"Setting the API key","text":"<p>You can set your API key at runtime like this:</p> <pre><code>import marvin\nmarvin.settings.openai.api_key = YOUR_API_KEY\n</code></pre> <p>However, it is preferable to pass sensitive settings as an environment variable: <code>MARVIN_OPENAI_API_KEY</code>. </p> <p>To set your OpenAI API key as an environment variable, open your terminal and run the following command, replacing  with the actual key: <pre><code>export MARVIN_OPENAI_API_KEY=&lt;your API key&gt;\n</code></pre> <p>This will set the key for the duration of your terminal session. To set it more permanently, configure your terminal or its respective env files.</p> <p>Using OpenAI standard API key locations</p> <p>For convenience, Marvin will respect the <code>OPENAI_API_KEY</code> environment variable or a key manually set as <code>openai.api_key</code> as long as no Marvin-specific keys were also provided.</p>"},{"location":"src/docs/configuration/openai/#using-a-model","title":"Using a model","text":"<p>Once your API key is set, you can use any valid OpenAI model by providing it as Marvin's <code>llm_model</code> setting: <pre><code>import marvin\nmarvin.settings.llm_model = 'gpt-4-0613'\n</code></pre></p> <p>Marvin will automatically recognize that the <code>gpt-3.5*</code> and <code>gpt-4*</code> families of models use the OpenAI provider. To indicate a provider explicitly, prefix the model name with <code>openai/</code>. For example: <code>marvin.settings.llm_model = 'openai/gpt-4'</code>.</p>"},{"location":"src/docs/configuration/settings/","title":"Settings","text":"<p>Marvin makes use of Pydantic's <code>BaseSettings</code> for configuration throughout the package.</p>"},{"location":"src/docs/configuration/settings/#environment-variables","title":"Environment Variables","text":"<p>All settings are configurable via environment variables like <code>MARVIN_&lt;setting name&gt;</code>.</p> <p>For example, in an <code>.env</code> file or in your shell config file you might have: <pre><code>MARVIN_LOG_LEVEL=DEBUG\nMARVIN_LLM_MODEL=gpt-4\nMARVIN_LLM_TEMPERATURE=0\n</code></pre></p>"},{"location":"src/docs/configuration/settings/#runtime-settings","title":"Runtime Settings","text":"<p>A runtime settings object is accessible via <code>marvin.settings</code> and can be used to access or update settings throughout the package.</p> <p>For example, to access or change the LLM model used by Marvin at runtime: <pre><code>import marvin\nmarvin.settings.llm_model\n# 'gpt-4'\nmarvin.settings.llm_model = 'gpt-3.5-turbo'\nmarvin.settings.llm_model\n# 'gpt-3.5-turbo'\n</code></pre></p>"},{"location":"src/docs/configuration/settings/#llm-providers","title":"LLM Providers","text":"<p>Marvin supports multiple LLM providers, including OpenAI and Anthropic. After configuring your credentials appropriately, you can use any supported model by setting <code>marvin.settings.llm_model</code> appropriately. </p> <p>Valid <code>llm_model</code> settings are strings with the form <code>\"{provider_key}/{model_name}\"</code>. For example, <code>\"openai/gpt-3.5-turbo\"</code>. </p> <p>For well-known models, you may provide the model name without a provider key. These models include:</p> <ul> <li>the <code>gpt-3.5-*</code> family from OpenAI</li> <li>the <code>gpt-4*</code> family from OpenAI</li> <li>the <code>claude-*</code> family from Anthropic</li> </ul> Provider Provider Key Models Notes OpenAI <code>openai</code> <code>gpt-3.5-turbo</code> (default), <code>gpt-4</code>, or any other compatible model Marvin is generally tested and optimized with this provider. Anthropic <code>anthropic</code> <code>claude-2</code>, <code>claude-instant-1</code> or any other compatible model"},{"location":"src/docs/prompts/executing/","title":"Executing","text":"In\u00a0[2]: Copied! <pre>from marvin.prompts.library import System, User, ChainOfThought\nfrom marvin.engine.language_models import chat_llm\nfrom typing import Optional\n\n\nclass ExpertSystem(System):\n    content: str = (\n        \"You are a world-class expert on {{topic}}. \"\n        \"When asked questions about {{topic}}, you answer correctly. \"\n        \"You only answer questions about {{topic}}. \"\n    )\n    topic: Optional[str]\n\n\nclass Tutor(System):\n    content: str = (\n        \"When you give an answer, you modulate your response based on the \"\n        \"inferred knowledge of the user. \"\n        \"Your student's name is {{name}}. \"\n    )\n    name: str = \"not provided\"\n\n\nmodel = chat_llm()\n\nresponse = await model(\n    (\n        ExpertSystem()\n        | Tutor()\n        | User(\n            \"I heard that there are types of geometries when the angles don't add up to\"\n            \" 180?\"\n        )\n        | ChainOfThought()\n    ).render(topic=\"geometry\", name=\"Adam\")\n)\n\nprint(response.content)\n</pre> from marvin.prompts.library import System, User, ChainOfThought from marvin.engine.language_models import chat_llm from typing import Optional   class ExpertSystem(System):     content: str = (         \"You are a world-class expert on {{topic}}. \"         \"When asked questions about {{topic}}, you answer correctly. \"         \"You only answer questions about {{topic}}. \"     )     topic: Optional[str]   class Tutor(System):     content: str = (         \"When you give an answer, you modulate your response based on the \"         \"inferred knowledge of the user. \"         \"Your student's name is {{name}}. \"     )     name: str = \"not provided\"   model = chat_llm()  response = await model(     (         ExpertSystem()         | Tutor()         | User(             \"I heard that there are types of geometries when the angles don't add up to\"             \" 180?\"         )         | ChainOfThought()     ).render(topic=\"geometry\", name=\"Adam\") )  print(response.content) <pre>Yes, you are correct! In traditional Euclidean geometry, the angles of a triangle always add up to 180 degrees. However, there are indeed other types of geometries where this is not the case. One such example is non-Euclidean geometry, which includes hyperbolic and elliptic geometries. In hyperbolic geometry, the angles of a triangle add up to less than 180 degrees, while in elliptic geometry, the angles add up to more than 180 degrees. These non-Euclidean geometries have their own unique properties and are studied in mathematics and physics.\n</pre> <p>Jargon Alert!</p> <p>     An `executor` here is a fancy phrase for a `while` or `for` loop under the context of a conversation with an LLM.     Below, `OpenAIExectutor` is a bare-bones implementation of 'if you get back a function call, call it, and pass the      answer back'.    </p> In\u00a0[\u00a0]: Copied! <pre>from marvin.prompts.library import System, ChainOfThought, User\nfrom marvin.engine.executors import OpenAIFunctionsExecutor\nfrom marvin.prompts import render_prompts\n\n\ndef write_code(language: str, code: str) -&gt; str:\n\"\"\"A function that writes code in `language` to accomplish task\"\"\"\n\n\nresponse = await OpenAIFunctionsExecutor(functions=[write_code]).start(\n    prompts=render_prompts(\n        System(content=\"You're an expert on {{subject}}.\")\n        | User(\n            content=\"I need to know how to write a function in {{subject}} to {{task}}\"\n        )\n        | ChainOfThought(),  # Tell the LLM to think step by step\n        {\"subject\": \"python\", \"task\": \"calculate the nth fibonacci number\"},\n    )\n)\n\nprint(response[-1].content)\n</pre> from marvin.prompts.library import System, ChainOfThought, User from marvin.engine.executors import OpenAIFunctionsExecutor from marvin.prompts import render_prompts   def write_code(language: str, code: str) -&gt; str:     \"\"\"A function that writes code in `language` to accomplish task\"\"\"   response = await OpenAIFunctionsExecutor(functions=[write_code]).start(     prompts=render_prompts(         System(content=\"You're an expert on {{subject}}.\")         | User(             content=\"I need to know how to write a function in {{subject}} to {{task}}\"         )         | ChainOfThought(),  # Tell the LLM to think step by step         {\"subject\": \"python\", \"task\": \"calculate the nth fibonacci number\"},     ) )  print(response[-1].content) In\u00a0[\u00a0]: Copied! <pre>\"\"\" \nHere is a Python function that calculates the nth Fibonacci number:\n\n```python\ndef fibonacci(n):\n    if n &lt;= 0:\n        return \"Invalid input\"\n    elif n == 1:\n        return 0\n    elif n == 2:\n        return 1\n    else:\n        return fibonacci(n-1) + fibonacci(n-2)\n```\n\nTo use this function, you can simply call it with the desired value of `n`:\n\n```python\nresult = fibonacci(5)\nprint(result)  # Output: 3\n```\n\nThis will calculate and print the 5th Fibonacci number, which is 3.\n\"\"\"\n</pre> \"\"\"  Here is a Python function that calculates the nth Fibonacci number:  ```python def fibonacci(n):     if n &lt;= 0:         return \"Invalid input\"     elif n == 1:         return 0     elif n == 2:         return 1     else:         return fibonacci(n-1) + fibonacci(n-2) ```  To use this function, you can simply call it with the desired value of `n`:  ```python result = fibonacci(5) print(result)  # Output: 3 ```  This will calculate and print the 5th Fibonacci number, which is 3. \"\"\""},{"location":"src/docs/prompts/executing/#executing-prompts","title":"Executing Prompts\u00b6","text":"<p>Marvin makes executing one-off <code>task</code> or <code>chain</code> patterns dead simple.</p>"},{"location":"src/docs/prompts/executing/#running-a-task","title":"Running a <code>task</code>\u00b6","text":"<p>Once you have a prompt defined, fire it off with your chosen LLM asyncronously like so:</p>"},{"location":"src/docs/prompts/executing/#running-a-chain","title":"Running a <code>chain</code>\u00b6","text":"<p>Of course, some applications require LLMs to run in an iterated loop so that it can deduce its next actions and take them. We've got you covered. Import an Executor (or create your own) and hit start.</p>"},{"location":"src/docs/prompts/writing/","title":"Writing","text":"In\u00a0[20]: Copied! <pre>from typing import Optional\nfrom marvin.prompts.library import System, User, ChainOfThought\n\n\nclass ExpertSystem(System):\n    content: str = (\n        \"You are a world-class expert on {{topic}}. \"\n        \"When asked questions about {{topic}}, you answer correctly.\"\n    )\n    topic: Optional[str]\n\n\nprompt = (\n    ExpertSystem(topic=\"python\")\n    | User(\"I need to know how to write a function to find the nth Fibonacci number.\")\n    | ChainOfThought()  # Tell the LLM to think step by step\n)\n\nprompt.dict()\n</pre> from typing import Optional from marvin.prompts.library import System, User, ChainOfThought   class ExpertSystem(System):     content: str = (         \"You are a world-class expert on {{topic}}. \"         \"When asked questions about {{topic}}, you answer correctly.\"     )     topic: Optional[str]   prompt = (     ExpertSystem(topic=\"python\")     | User(\"I need to know how to write a function to find the nth Fibonacci number.\")     | ChainOfThought()  # Tell the LLM to think step by step )  prompt.dict() Out[20]: <pre>[{'role': 'system',\n  'content': 'You are a world-class expert on python. When asked questions about python, you answer correctly.'},\n {'role': 'user',\n  'content': 'I need to know how to write a function to find the nth Fibonacci number.'},\n {'role': 'assistant', 'content': \"Let's think step by step.\"}]</pre> <p>A note of gratitude</p> <p>     The `piping` convention above was inspired by, and built in collaboration with, our friend Jason Liu.   </p> In\u00a0[21]: Copied! <pre>from typing import Optional\nfrom marvin.prompts.library import System, User, ChainOfThought\n\n\nclass ExpertSystem(System):\n    content: str = (\n        \"You are a world-class expert on {{topic}}. \"\n        \"When asked questions about {{topic}}, you answer correctly.\"\n    )\n    topic: Optional[str]\n\n\nprompt = (\n    ExpertSystem()\n    | User(\n        \"I need to know how to write a function in {{topic}} to find the nth Fibonacci\"\n        \" number.\"\n    )\n    | ChainOfThought()  # Tell the LLM to think step by step\n)\n\nprompt.dict(topic=\"rust\")\n</pre> from typing import Optional from marvin.prompts.library import System, User, ChainOfThought   class ExpertSystem(System):     content: str = (         \"You are a world-class expert on {{topic}}. \"         \"When asked questions about {{topic}}, you answer correctly.\"     )     topic: Optional[str]   prompt = (     ExpertSystem()     | User(         \"I need to know how to write a function in {{topic}} to find the nth Fibonacci\"         \" number.\"     )     | ChainOfThought()  # Tell the LLM to think step by step )  prompt.dict(topic=\"rust\") Out[21]: <pre>[{'role': 'system',\n  'content': 'You are a world-class expert on rust. When asked questions about rust, you answer correctly.'},\n {'role': 'user',\n  'content': 'I need to know how to write a function in rust to find the nth Fibonacci number.'},\n {'role': 'assistant', 'content': \"Let's think step by step.\"}]</pre> <p>In this scenario, the prompt sets up a simulated conversation where the system establishes the user's expertise level in Python. The user then expresses their need to understand how to write a function in Python. To facilitate a step-by-step thought process, the ChainOfThought() function is used.</p> In\u00a0[\u00a0]: Copied! <pre>from marvin.prompts.library import System\n\n\nclass ReActPattern(System):\n    content = \"\"\"\n    You run in a loop of Thought, Action, PAUSE, Observation.\n    At the end of the loop you output an Answer\n    Use Thought to describe your thoughts about the question you have been asked.\n    Use Action to run one of the actions available to you - then return PAUSE.\n    Observation will be the result of running those actions.\n  \"\"\"\n</pre> from marvin.prompts.library import System   class ReActPattern(System):     content = \"\"\"     You run in a loop of Thought, Action, PAUSE, Observation.     At the end of the loop you output an Answer     Use Thought to describe your thoughts about the question you have been asked.     Use Action to run one of the actions available to you - then return PAUSE.     Observation will be the result of running those actions.   \"\"\" <p>Marvin provides simple, opinionated components so that you can subclass and customize prompts the same way you would for code.</p> In\u00a0[45]: Copied! <pre>import pydantic\nfrom marvin.prompts.library import System\n\n\nclass ColumnInfo(pydantic.BaseModel):\n    name: str\n    description: str\n\n\nclass SQLTableDescription(System):\n    content = \"\"\"\n    If you chose to, you may query a table whose schema is defined below:\n    {% for column in columns %}\n    - {{ column.name }}: {{ column.description }}\n    {% endfor %}\n    \"\"\"\n\n    columns: list[ColumnInfo] = pydantic.Field(\n        ..., description=\"name, description pairs of SQL Schema\"\n    )\n\n\nUserQueryPrompt = SQLTableDescription(\n    columns=[\n        ColumnInfo(name=\"last_login\", description=\"Date and time of user's last login\"),\n        ColumnInfo(\n            name=\"date_created\",\n            description=\"Date and time when the user record was created\",\n        ),\n        ColumnInfo(\n            name=\"date_last_purchase\",\n            description=\"Date and time of user's last purchase\",\n        ),\n    ]\n)\n\nprint(UserQueryPrompt.read())\n</pre> import pydantic from marvin.prompts.library import System   class ColumnInfo(pydantic.BaseModel):     name: str     description: str   class SQLTableDescription(System):     content = \"\"\"     If you chose to, you may query a table whose schema is defined below:          {% for column in columns %}     - {{ column.name }}: {{ column.description }}     {% endfor %}     \"\"\"      columns: list[ColumnInfo] = pydantic.Field(         ..., description=\"name, description pairs of SQL Schema\"     )   UserQueryPrompt = SQLTableDescription(     columns=[         ColumnInfo(name=\"last_login\", description=\"Date and time of user's last login\"),         ColumnInfo(             name=\"date_created\",             description=\"Date and time when the user record was created\",         ),         ColumnInfo(             name=\"date_last_purchase\",             description=\"Date and time of user's last purchase\",         ),     ] )  print(UserQueryPrompt.read()) <pre>If you chose to, you may query a table whose schema is defined below:\n\n- last_login: Date and time of user's last login\n- date_created: Date and time when the user record was created\n- date_last_purchase: Date and time of user's last purchase\n\n</pre>"},{"location":"src/docs/prompts/writing/#creating-prompts","title":"Creating Prompts\u00b6","text":"<p>Marvin lets you define dynamic prompts using code, eliminating the need for cumbersome template management. With this approach, you can easily create reusable and modular prompts, streamlining the development process.</p>"},{"location":"src/docs/prompts/writing/#templating-prompts","title":"Templating Prompts\u00b6","text":"<p>In many applications, templating is unavoidable. In these cases, Marvin's optional templating engine simplifies the process of sharing context across prompts to an unprecedented level. By passing native Python types or Pydantic objects into the rendering engine, you can seamlessly establish context for entire conversations. This feature enables effortless information flow and context continuity throughout the prompt interactions.</p>"},{"location":"src/docs/prompts/writing/#rendering-prompts","title":"Rendering Prompts\u00b6","text":"<p>A chain of prompts is turned into messages with the <code>render_prompts</code> function. This function has a few responsibilities in addition to generating messages: it renders templates using runtime variables, sorts messages, and trims prompts to fit into a model's context window. The last two actions depend on the optional <code>position</code> and <code>priority</code> attributes of each prompt.</p> <p>For example, the <code>System</code> prompt defines <code>position=0</code> and <code>priority=1</code> to indicate that system prompts should be rendered first and given high priority when trimming the context. As an optimization, <code>render_prompt</code> automatically combines multiple system messages into a single message.</p> <p><code>ChainOfThought()</code> has a <code>position=-1</code> to indicate it should be the last message. If position is not set explicitly, then prompts will take the order they are added to the chain.</p>"},{"location":"src/docs/utilities/openai/","title":"OpenAI API","text":"In\u00a0[\u00a0]: Copied! <pre>import marvin\n\nmarvin.openai.ChatCompletion(model=\"gpt-3.5-turbo\").create(\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke!\"}]\n)\n</pre> import marvin  marvin.openai.ChatCompletion(model=\"gpt-3.5-turbo\").create(     messages=[{\"role\": \"user\", \"content\": \"Tell me a joke!\"}] ) <p>This small quality-of-life change lets you do some pretty great things, like:</p> In\u00a0[\u00a0]: Copied! <pre>import marvin\n\n# Define a smart model.\ngpt3 = marvin.openai.ChatCompletion(model=\"gpt-3.5-turbo\")\n\n# Define a smarter model.\ngpt4 = marvin.openai.ChatCompletion(model=\"gpt-4\")\n\n# gp4.create(**kwargs) ~ openai.ChatCompletion.create(model = 'gpt-4', **kwargs)\n</pre> import marvin  # Define a smart model. gpt3 = marvin.openai.ChatCompletion(model=\"gpt-3.5-turbo\")  # Define a smarter model. gpt4 = marvin.openai.ChatCompletion(model=\"gpt-4\")  # gp4.create(**kwargs) ~ openai.ChatCompletion.create(model = 'gpt-4', **kwargs) <p>Above, each instance passes its respective <code>model</code> keyword argument to create and acreate on each invocation.</p> In\u00a0[\u00a0]: Copied! <pre>import marvin\n\npirate_system_message = {\"role\": \"system\", \"content\": \"You talk like a pirate\"}\n\n# Define a smart, public model.\nsmart_pirate = gpt3(messages=[pirate_system_message])\n\nsmart_pirate.create(messages=[{\"role\": \"user\", \"content\": \"Hello!\"}])\n</pre> import marvin  pirate_system_message = {\"role\": \"system\", \"content\": \"You talk like a pirate\"}  # Define a smart, public model. smart_pirate = gpt3(messages=[pirate_system_message])  smart_pirate.create(messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]) <p>By default, keyword arguments in create and acreate override frozen parameters. This default has two exceptions: messages and functions, wherein passed messages are concatenated with frozen messages (and likewise for functions).</p> In\u00a0[\u00a0]: Copied! <pre>from marvin.openai import openai_fn\n\n\n@openai_fn\ndef add(x: int, y: int) -&gt; str:\n    return x + y\n</pre> from marvin.openai import openai_fn   @openai_fn def add(x: int, y: int) -&gt; str:     return x + y In\u00a0[\u00a0]: Copied! <pre>from marvin.openai import openai_fn\n\n\n@openai_fn\ndef add(x: int, y: int) -&gt; str:\n\"\"\"Adds two numbers together\"\"\"\n    return x + y\n\n\nadd.schema\n</pre> from marvin.openai import openai_fn   @openai_fn def add(x: int, y: int) -&gt; str:     \"\"\"Adds two numbers together\"\"\"     return x + y   add.schema <p>Returns its JSON Schema to use with OpenAI's function API.</p> In\u00a0[\u00a0]: Copied! <pre>{\n    \"name\": \"add\",\n    \"description\": \"Adds two numbers together\",\n    \"parameters\": {\n        \"x\": {\"type\": \"int\", \"description\": null},\n        \"y\": {\"type\": \"int\", \"description\": null},\n        \"required\": [\"x\", \"y\"],\n    },\n}\n</pre> {     \"name\": \"add\",     \"description\": \"Adds two numbers together\",     \"parameters\": {         \"x\": {\"type\": \"int\", \"description\": null},         \"y\": {\"type\": \"int\", \"description\": null},         \"required\": [\"x\", \"y\"],     }, } In\u00a0[\u00a0]: Copied! <pre>import openai\nfrom marvin.openai import openai_fn\n\n\n@openai_fn\ndef add(x: int, y: int) -&gt; str:\n\"\"\"Adds two numbers together\"\"\"\n    return x + y\n\n\nresponse = openai.ChatCompletion.create(\n    messages=[{\"role\": \"user\", \"content\": \"What is 123123 + 85858?\"}]\n)\n\nadd.from_response(response) == 208981\n</pre> import openai from marvin.openai import openai_fn   @openai_fn def add(x: int, y: int) -&gt; str:     \"\"\"Adds two numbers together\"\"\"     return x + y   response = openai.ChatCompletion.create(     messages=[{\"role\": \"user\", \"content\": \"What is 123123 + 85858?\"}] )  add.from_response(response) == 208981 In\u00a0[\u00a0]: Copied! <pre>from marvin.openai import OpenAIFunctionRegistry\n\nregistry = OpenAIFunctionRegistry()\n\n\n@registry.register\ndef add(x: int, y: int) -&gt; int:\n\"\"\"Adds two numbers together\"\"\"\n    return x + y\n\n\n@registry.register\ndef subtract(x: int, y: int) -&gt; int:\n\"\"\"Subtracts `y` from `x`\"\"\"\n    return x - y\n\n\nregistry.schema\n</pre> from marvin.openai import OpenAIFunctionRegistry  registry = OpenAIFunctionRegistry()   @registry.register def add(x: int, y: int) -&gt; int:     \"\"\"Adds two numbers together\"\"\"     return x + y   @registry.register def subtract(x: int, y: int) -&gt; int:     \"\"\"Subtracts `y` from `x`\"\"\"     return x - y   registry.schema <p>Which yields a list of schemas which can be passed as a keyword argument to OpenAI's SDK.</p> In\u00a0[\u00a0]: Copied! <pre>[\n    {\n        \"name\": \"add\",\n        \"description\": \"Adds two numbers together\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"x\": {\"title\": \"X\", \"type\": \"integer\"},\n                \"y\": {\"title\": \"Y\", \"type\": \"integer\"},\n            },\n            \"required\": [\"x\", \"y\"],\n        },\n    },\n    {\n        \"name\": \"subtract\",\n        \"description\": \"Subtracts `y` from `x`\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"x\": {\"title\": \"X\", \"type\": \"integer\"},\n                \"y\": {\"title\": \"Y\", \"type\": \"integer\"},\n            },\n            \"required\": [\"x\", \"y\"],\n        },\n    },\n]\n</pre> [     {         \"name\": \"add\",         \"description\": \"Adds two numbers together\",         \"parameters\": {             \"type\": \"object\",             \"properties\": {                 \"x\": {\"title\": \"X\", \"type\": \"integer\"},                 \"y\": {\"title\": \"Y\", \"type\": \"integer\"},             },             \"required\": [\"x\", \"y\"],         },     },     {         \"name\": \"subtract\",         \"description\": \"Subtracts `y` from `x`\",         \"parameters\": {             \"type\": \"object\",             \"properties\": {                 \"x\": {\"title\": \"X\", \"type\": \"integer\"},                 \"y\": {\"title\": \"Y\", \"type\": \"integer\"},             },             \"required\": [\"x\", \"y\"],         },     }, ] In\u00a0[\u00a0]: Copied! <pre>from marvin.openai import OpenAIFunctionRegistry\n\nregistry = OpenAIFunctionRegistry()\n\n\n@registry.register\ndef add(x: int, y: int) -&gt; int:\n\"\"\"Adds two numbers together\"\"\"\n    return x + y\n\n\n@registry.register\ndef subtract(x: int, y: int) -&gt; int:\n\"\"\"Subtracts `y` from `x`\"\"\"\n    return x - y\n\n\nresponse = openai.ChatCompletion.create(\n    functions=registry.schema,\n    messages=[{\"role\": \"user\", \"content\": \"What is 123123 - 85858?\"}],\n)\n\nregistry.from_response(response).get(\"content\") == 37265\n</pre> from marvin.openai import OpenAIFunctionRegistry  registry = OpenAIFunctionRegistry()   @registry.register def add(x: int, y: int) -&gt; int:     \"\"\"Adds two numbers together\"\"\"     return x + y   @registry.register def subtract(x: int, y: int) -&gt; int:     \"\"\"Subtracts `y` from `x`\"\"\"     return x - y   response = openai.ChatCompletion.create(     functions=registry.schema,     messages=[{\"role\": \"user\", \"content\": \"What is 123123 - 85858?\"}], )  registry.from_response(response).get(\"content\") == 37265 In\u00a0[\u00a0]: Copied! <pre>math = OpenAIFunctionRegistry()\n\narithmetic = OpenAIFunctionRegistry()\n\ntrigonometry = OpenAIFunctionRegistry()\n\n\n@arithmetic.register\ndef add(x: int, y: int) -&gt; int:\n\"\"\"Adds two numbers together\"\"\"\n\n\n@trigonometry.register\ndef tan(theta: float) -&gt; float:\n\"\"\"Calculates the tangent of `theta`.\"\"\"\n    return x - y\n\n\nmath.include(arithmetic)\nmath.include(trigonometry)\n\nmath.schema\n</pre> math = OpenAIFunctionRegistry()  arithmetic = OpenAIFunctionRegistry()  trigonometry = OpenAIFunctionRegistry()   @arithmetic.register def add(x: int, y: int) -&gt; int:     \"\"\"Adds two numbers together\"\"\"   @trigonometry.register def tan(theta: float) -&gt; float:     \"\"\"Calculates the tangent of `theta`.\"\"\"     return x - y   math.include(arithmetic) math.include(trigonometry)  math.schema In\u00a0[\u00a0]: Copied! <pre>{\n    \"functions\": [\n        {\n            \"name\": \"add\",\n            \"description\": \"Adds two numbers together\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"x\": {\"title\": \"X\", \"type\": \"integer\"},\n                    \"y\": {\"title\": \"Y\", \"type\": \"integer\"},\n                },\n                \"required\": [\"x\", \"y\"],\n            },\n        },\n        {\n            \"name\": \"tan\",\n            \"description\": \"Calculates the tangent of `theta`.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\"theta\": {\"title\": \"Theta\", \"type\": \"number\"}},\n                \"required\": [\"theta\"],\n            },\n        },\n    ],\n    \"function_call\": \"auto\",\n}\n</pre> {     \"functions\": [         {             \"name\": \"add\",             \"description\": \"Adds two numbers together\",             \"parameters\": {                 \"type\": \"object\",                 \"properties\": {                     \"x\": {\"title\": \"X\", \"type\": \"integer\"},                     \"y\": {\"title\": \"Y\", \"type\": \"integer\"},                 },                 \"required\": [\"x\", \"y\"],             },         },         {             \"name\": \"tan\",             \"description\": \"Calculates the tangent of `theta`.\",             \"parameters\": {                 \"type\": \"object\",                 \"properties\": {\"theta\": {\"title\": \"Theta\", \"type\": \"number\"}},                 \"required\": [\"theta\"],             },         },     ],     \"function_call\": \"auto\", } <p>Code Generation</p> <p>Marvin offers an experimental utility to author code using OpenAI's function API. Given a function with a typed signature and a docstring, we can write entire functions in the language of your choice. Simple call <code>.code()</code> on a function decorated with @openai_fn.</p> <p>Behind the scenes, we define a utility function write_code:</p> In\u00a0[\u00a0]: hide_cell Copied! <pre>from marvin.openai import openai_fn\n\n\n@openai_fn\ndef write_code(\n    language: str,\n    filename: str,\n    name: str,\n    docstring: str,\n    code: str,\n) -&gt; str:\n\"\"\"Accepts and checks expertly staff engineer quality written `code` in `language`\"\"\"\n    return (language, filename, name, docstring, code)\n</pre> from marvin.openai import openai_fn   @openai_fn def write_code(     language: str,     filename: str,     name: str,     docstring: str,     code: str, ) -&gt; str:     \"\"\"Accepts and checks expertly staff engineer quality written `code` in `language`\"\"\"     return (language, filename, name, docstring, code) <p>When you call the <code>code</code> method on your openai_fn, we simply call:</p> In\u00a0[\u00a0]: hide_cell Copied! <pre>@openai_fn\ndef add(x: int, y: int) -&gt; str:\n\"\"\"Adds two numbers together\"\"\"\n    # There is no code here! #\n\n\n{\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": f\"\"\"A function in python that described by the following schema:\\n {add.schema}\"\"\",\n        },\n    ],\n    **write_code.schema,\n}\n</pre> @openai_fn def add(x: int, y: int) -&gt; str:     \"\"\"Adds two numbers together\"\"\"     # There is no code here! #   {     \"messages\": [         {             \"role\": \"user\",             \"content\": f\"\"\"A function in python that described by the following schema:\\n {add.schema}\"\"\",         },     ],     **write_code.schema, } <p>This returns the following payload for you to send to OpenAI, which forces it to write code that satisfies the high-level description of your function.</p> In\u00a0[\u00a0]: hide_cell Copied! <pre>{\n    'messages': \n    [{\n        'role': 'user',\n        'content': \"A function in python that described by the following schema:\n            {'name': 'add', \n             'description': 'Adds two numbers together', \n             'parameters': {\n                 'type': 'object', \n                 'properties': {\n                     'x': {'title': 'X', 'type': 'integer'}, \n                     'y': {'title': 'Y', 'type': 'integer'}\n                }, 'required': ['x', 'y']\n            }\n        }\"\n    }],\n    'functions': [\n        {\n            'name': 'write_code',\n            'description': 'Accepts and checks expertly staff engineer quality written `code` in `language`',\n            'parameters': {\n                'type': 'object',\n                'properties': {\n                    'language': {'title': 'Language', 'type': 'string'},\n                    'filename': {'title': 'Filename', 'type': 'string'},\n                    'name': {'title': 'Name', 'type': 'string'},\n                    'docstring': {'title': 'Docstring', 'type': 'string'},\n                    'code': {'title': 'Code', 'type': 'string'}},\n            'required': ['language', 'filename', 'name', 'docstring', 'code']\n            }\n        }\n    ],\n    'function_call': {'name': 'write_code'}\n}\n</pre> {     'messages':      [{         'role': 'user',         'content': \"A function in python that described by the following schema:             {'name': 'add',               'description': 'Adds two numbers together',               'parameters': {                  'type': 'object',                   'properties': {                      'x': {'title': 'X', 'type': 'integer'},                       'y': {'title': 'Y', 'type': 'integer'}                 }, 'required': ['x', 'y']             }         }\"     }],     'functions': [         {             'name': 'write_code',             'description': 'Accepts and checks expertly staff engineer quality written `code` in `language`',             'parameters': {                 'type': 'object',                 'properties': {                     'language': {'title': 'Language', 'type': 'string'},                     'filename': {'title': 'Filename', 'type': 'string'},                     'name': {'title': 'Name', 'type': 'string'},                     'docstring': {'title': 'Docstring', 'type': 'string'},                     'code': {'title': 'Code', 'type': 'string'}},             'required': ['language', 'filename', 'name', 'docstring', 'code']             }         }     ],     'function_call': {'name': 'write_code'} }  <p>If we give it a trivial example and call <code>.code()</code>, we see:</p> In\u00a0[\u00a0]: hide_cell Copied! <pre>from marvin.openai import openai_fn\nimport openai\n\nopenai.api_key = \"YOUR_OPENAPI_KEY\"\n\n\n@openai_fn\ndef add(x: int, y: int) -&gt; str:\n\"\"\"Adds two numbers together\"\"\"\n    # There is no code here! #\n\n\nwrite_code_instructions = add.code()\n\nresponse = await openai.ChatCompletion.acreate(\n    model=\"gpt-3.5-turbo\", **write_code_instructions\n)\n\nprint(response.choices[0].message.get(\"function_call\").get(\"arguments\"))\n</pre> from marvin.openai import openai_fn import openai  openai.api_key = \"YOUR_OPENAPI_KEY\"   @openai_fn def add(x: int, y: int) -&gt; str:     \"\"\"Adds two numbers together\"\"\"     # There is no code here! #   write_code_instructions = add.code()  response = await openai.ChatCompletion.acreate(     model=\"gpt-3.5-turbo\", **write_code_instructions )  print(response.choices[0].message.get(\"function_call\").get(\"arguments\")) <p>Which returns</p> In\u00a0[\u00a0]: hide_cell Copied! <pre>{\n    \"language\": \"python\",\n    \"filename\": \"add.py\",\n    \"name\": \"add\",\n    \"docstring\": \"Adds two numbers together\",\n    \"code\": \"def add(x: int, y: int) -&gt; int:\\n    return x + y\",\n}\n</pre> {     \"language\": \"python\",     \"filename\": \"add.py\",     \"name\": \"add\",     \"docstring\": \"Adds two numbers together\",     \"code\": \"def add(x: int, y: int) -&gt; int:\\n    return x + y\", }"},{"location":"src/docs/utilities/openai/#openai","title":"OpenAI\u00b6","text":"<p>Marvin includes first-class utilities for working with OpenAI's API. It's familiar, cuts boiler-plate, and - most importantly - optional. It's the type of stuff you'd build after your first or second refactor. Fingers crossed it saves you as much dev time as it's saved us.</p> <p>If you're looking for something higher-level, check out the rest of the docs. This section is for folks who prefer to work with a lower-level API but still appreciate a little syntactic sugar.</p>"},{"location":"src/docs/utilities/openai/#chatcompletion","title":"ChatCompletion\u00b6","text":"<p>Marvin includes a subclass of OpenAI's SDK implementation of ChatCompletion. This utility is completely standalone: you're free to use it with or without Marvin's other components or framework.</p>"},{"location":"src/docs/utilities/openai/#frozen-keyword-arguments","title":"Frozen Keyword Arguments\u00b6","text":"<p>In <code>openai.ChatCompletion</code>, you normally have to pass keyword arguments to each invocation of <code>create</code> and <code>acreate</code>. With Marvin, you can choose to pass these keywords to the constructor of <code>marvin.openai.ChatCompletion</code>, and have those keywords passed to each subsequent invocation. This lets you do stuff like:</p>"},{"location":"src/docs/utilities/openai/#creating-model-facets","title":"Creating Model Facets\u00b6","text":"<p>By freezing keyword arguments this lets us define, premission, and version <code>facets</code> of ChatGPT. This let's us define separate model instances for internal, public, or customer use. By combining this with function calling (see below), we can also give smart models access to smarter models when they intuit that it's prudent.</p>"},{"location":"src/docs/utilities/openai/#facet-inheritence","title":"Facet Inheritence\u00b6","text":"<p>Of course, these model configurations are chainable.</p>"},{"location":"src/docs/utilities/openai/#functions","title":"Functions\u00b6","text":"<p>OpenAI's ChatCompletion API enables you to pass a list of <code>functions</code> for it to optionally call in service of a query. If it chooses to execute a function, either by choice or instruction, it will return the function's name along with its formatted parameters for you to evaluate. The OpenAI schema accepts a JSON Schema representation of your functions.</p> <p>Marvin includes lightweight utilities for working with OpenAI's function API. These utilities are completely standalone: you're free to use them with or without Marvin's other components or framework. We expose <code>openai_fn</code>, a function decorator that makes working with OpenAI functions straightforward.</p>"},{"location":"src/docs/utilities/openai/#serialization","title":"Serialization\u00b6","text":"<p>Marvin allows auto creation of JSON Schemas from functions:</p>"},{"location":"src/docs/utilities/openai/#evaluation","title":"Evaluation\u00b6","text":"<p>When ChatGPT decides it needs to invoke a function in service of your query, it will return the name of the function it would like you to invoke and the arguments to evaluate it with. Marvin provides means to evaluate a function from a ChatGPT Function Call Response.</p>"},{"location":"src/docs/utilities/openai/#registry","title":"Registry\u00b6","text":"<p>In most function-calling applications, you'll want to pass a list of several functions. The developer experience, accordingly, gets <code>n</code> times worse. We introduce a standard Function Registry to make things a little easier. It doubles as an API Router if you're into that sort of thing.</p>"},{"location":"src/docs/utilities/openai/#serialization","title":"Serialization\u00b6","text":""},{"location":"src/docs/utilities/openai/#evaluation","title":"Evaluation\u00b6","text":"<p>When ChatGPT decides it needs to invoke one of the functions in your registry in service of your query, it will return the name of the function it would like you to invoke and the arguments to evaluate it with. Marvin provides means to evaluate a function call in your registry dictated by a ChatGPT Function Call Response.</p>"},{"location":"src/docs/utilities/openai/#composability","title":"Composability\u00b6","text":"<p>Given two function routers, you can easily compose them. This lets you separately define, say, one registry devoted to accessing and processing one data source, and another devoted to accessing and processing another (with stricter permissions, perhaps). Including them is as straightfoward as calling <code>include</code> (which is, of course, idempotent).</p>"},{"location":"src/getting_started/hello_marvin/","title":"Hello Marvin \ud83d\udc4b","text":"<p>A few quick examples to get you started with Marvin:</p>"},{"location":"src/getting_started/hello_marvin/#natural-language-pydantic-models","title":"natural language &gt;&gt; <code>pydantic</code> models","text":"<pre><code>from pydantic import BaseModel\nfrom marvin import ai_model\n@ai_model\nclass WorldCity(BaseModel):\nname: str\ncountry: str\nlat: float\nlon: float\n@property\ndef map_url(self) -&gt; str:\nreturn (\n\"https://findlatitudeandlongitude.com/\"\nf\"?lat={self.lat}&amp;lon={self.lon}\"\n)\ncity = WorldCity(\"city that never sleeps\")\nprint(city.map_url)\n# https://findlatitudeandlongitude.com/?lat=40.7128&amp;lon=-74.006\n</code></pre>"},{"location":"src/getting_started/hello_marvin/#classify-anything","title":"classify anything","text":"<pre><code>from enum import Enum\nfrom marvin import ai_classifier\n@ai_classifier\nclass ZodiacSign(Enum):\nARIES = \"Aries\"\nTAURUS = \"Taurus\"\nGEMINI = \"Gemini\"\nCANCER = \"Cancer\"\nLEO = \"Leo\"\nVIRGO = \"Virgo\"\nLIBRA = \"Libra\"\nSCORPIO = \"Scorpio\"\nSAGITTARIUS = \"Sagittarius\"\nCAPRICORN = \"Capricorn\"\nAQUARIUS = \"Aquarius\"\nPISCES = \"Pisces\"    \nZodiacSign(\n\"creative, compassionate, always listening to Marvin's room by Drake\"\n)\n# &lt;ZodiacSign.PISCES: 'Pisces'&gt;\n</code></pre>"},{"location":"src/getting_started/hello_marvin/#functional-prompts-with-type-safe-results","title":"functional prompts with type-safe results","text":"<pre><code>from typing import Optional\nfrom pydantic import BaseModel\nfrom marvin import ai_fn\nclass Person(BaseModel):\nname: str\nyear_of_death: Optional[int] = None\n@ai_fn\ndef list_people(context: str) -&gt; list[Person]:\n\"\"\" List people given a context \"\"\"\nlist_people(\n\"was mentored by the great educator in\"\n\" youth before hosting his own show Cosmos\"\n)\n# [\n#   Person(name='Neil deGrasse Tyson', year_of_death=None),\n#   Person(name='Carl Sagan', year_of_death=1996)\n# ]\n</code></pre>"},{"location":"src/getting_started/installation/","title":"Installation","text":""},{"location":"src/getting_started/installation/#basic-installation","title":"Basic Installation","text":"<p>You can install Marvin with <code>pip</code> (note that Marvin requires Python 3.9+):</p> <pre><code>pip install marvin\n</code></pre> <p>To verify your installation, run <code>marvin --help</code> in your terminal. </p> <p>You can upgrade to the latest released version at any time:</p> <pre><code>pip install marvin -U\n</code></pre> <p>Breaking changes in 1.0</p> <p>Please note that Marvin 1.0 introduces a number of breaking changes and is not compatible with Marvin 0.X.</p>"},{"location":"src/getting_started/installation/#adding-optional-dependencies","title":"Adding Optional Dependencies","text":"<p>Marvin's base install is designed to be as lightweight as possible, with minimal dependencies. To use functionality that interacts with other services, install Marvin with any required optional dependencies. For example, to use Anthropic models, install Marvin with the optional Anthropic provider:</p> <pre><code>pip install 'marvin[anthropic]'\n</code></pre>"},{"location":"src/getting_started/installation/#installing-for-development","title":"Installing for Development","text":"<p>See the contributing docs for instructions on installing Marvin for development.</p>"},{"location":"src/getting_started/quickstart/","title":"Quickstart","text":"In\u00a0[\u00a0]: Copied! <pre>import marvin\n\n# to use an OpenAI model (if not specified, defaults to gpt-3.5-turbo)\nmarvin.settings.openai.api_key = YOUR_API_KEY\n</pre> import marvin  # to use an OpenAI model (if not specified, defaults to gpt-3.5-turbo) marvin.settings.openai.api_key = YOUR_API_KEY <p>To use another provider or model, please see the configuration docs.</p> In\u00a0[2]: Copied! <pre>from marvin import ai_model\nfrom pydantic import BaseModel, Field\n\n\n@ai_model\nclass Location(BaseModel):\n    city: str\n    state: str = Field(..., description=\"The two-letter state abbreviation\")\n\n\nLocation(\"The Big Apple\")\n</pre> from marvin import ai_model from pydantic import BaseModel, Field   @ai_model class Location(BaseModel):     city: str     state: str = Field(..., description=\"The two-letter state abbreviation\")   Location(\"The Big Apple\") Out[2]: <pre>Location(city='New York', state='NY')</pre> In\u00a0[4]: Copied! <pre>from marvin import ai_classifier\nfrom enum import Enum\n\n\n@ai_classifier\nclass AppRoute(Enum):\n\"\"\"Represents distinct routes command bar for a different application\"\"\"\n\n    USER_PROFILE = \"/user-profile\"\n    SEARCH = \"/search\"\n    NOTIFICATIONS = \"/notifications\"\n    SETTINGS = \"/settings\"\n    HELP = \"/help\"\n    CHAT = \"/chat\"\n    DOCS = \"/docs\"\n    PROJECTS = \"/projects\"\n    WORKSPACES = \"/workspaces\"\n\n\nAppRoute(\"update my name\")\n</pre> from marvin import ai_classifier from enum import Enum   @ai_classifier class AppRoute(Enum):     \"\"\"Represents distinct routes command bar for a different application\"\"\"      USER_PROFILE = \"/user-profile\"     SEARCH = \"/search\"     NOTIFICATIONS = \"/notifications\"     SETTINGS = \"/settings\"     HELP = \"/help\"     CHAT = \"/chat\"     DOCS = \"/docs\"     PROJECTS = \"/projects\"     WORKSPACES = \"/workspaces\"   AppRoute(\"update my name\") Out[4]: <pre>&lt;AppRoute.USER_PROFILE: '/user-profile'&gt;</pre> In\u00a0[9]: Copied! <pre>from marvin import ai_fn\n\n\n@ai_fn\ndef sentiment(text: str) -&gt; float:\n\"\"\"\n    Given `text`, returns a number between 1 (positive) and -1 (negative)\n    indicating its sentiment score.\n    \"\"\"\n\n\nprint(\"Text 1:\", sentiment(\"I love working with Marvin!\"))\nprint(\"Text 2:\", sentiment(\"These examples could use some work...\"))\n</pre> from marvin import ai_fn   @ai_fn def sentiment(text: str) -&gt; float:     \"\"\"     Given `text`, returns a number between 1 (positive) and -1 (negative)     indicating its sentiment score.     \"\"\"   print(\"Text 1:\", sentiment(\"I love working with Marvin!\")) print(\"Text 2:\", sentiment(\"These examples could use some work...\")) <pre>Text 1: 0.8\nText 2: -0.2\n</pre> <p>Because AI functions are just like regular functions, you can quickly modify them for your needs. Here, we modify the above example to work with multiple strings at once:</p> In\u00a0[11]: Copied! <pre>from marvin import ai_fn\n\n\n@ai_fn\ndef sentiment_list(texts: list[str]) -&gt; list[float]:\n\"\"\"\n    Given a list of `texts`, returns a list of numbers between 1 (positive) and\n    -1 (negative) indicating their respective sentiment scores.\n    \"\"\"\n\n\nsentiment_list(\n    [\n        \"That was surprisingly easy!\",\n        \"Oh no, not again.\",\n    ]\n)\n</pre> from marvin import ai_fn   @ai_fn def sentiment_list(texts: list[str]) -&gt; list[float]:     \"\"\"     Given a list of `texts`, returns a list of numbers between 1 (positive) and     -1 (negative) indicating their respective sentiment scores.     \"\"\"   sentiment_list(     [         \"That was surprisingly easy!\",         \"Oh no, not again.\",     ] ) Out[11]: <pre>[0.7, -0.5]</pre> In\u00a0[1]: Copied! <pre>from datetime import datetime\nfrom pydantic import BaseModel, Field\nfrom marvin import AIApplication\n\n\n# create models to represent the state of our ToDo app\nclass ToDo(BaseModel):\n    title: str\n    description: str = None\n    due_date: datetime = None\n    done: bool = False\n\n\nclass ToDoState(BaseModel):\n    todos: list[ToDo] = []\n\n\n# create the app with an initial state and description\ntodo_app = AIApplication(\n    state=ToDoState(),\n    description=(\n        \"A simple todo app. Users will provide instructions for creating and updating\"\n        \" their todo lists.\"\n    ),\n)\n</pre> from datetime import datetime from pydantic import BaseModel, Field from marvin import AIApplication   # create models to represent the state of our ToDo app class ToDo(BaseModel):     title: str     description: str = None     due_date: datetime = None     done: bool = False   class ToDoState(BaseModel):     todos: list[ToDo] = []   # create the app with an initial state and description todo_app = AIApplication(     state=ToDoState(),     description=(         \"A simple todo app. Users will provide instructions for creating and updating\"         \" their todo lists.\"     ), ) <p>Now we can invoke the app directly to add a to-do item. Note that the app understands that it is supposed to manipulate state, not just respond conversationally.</p> In\u00a0[2]: Copied! <pre># invoke the application by adding a todo\nresponse = todo_app(\"I need to go to the store tomorrow at 5pm\")\n\n\nprint(\n    f\"Response: {response.content}\\n\",\n)\nprint(f\"App state: {todo_app.state.json(indent=2)}\")\n</pre> # invoke the application by adding a todo response = todo_app(\"I need to go to the store tomorrow at 5pm\")   print(     f\"Response: {response.content}\\n\", ) print(f\"App state: {todo_app.state.json(indent=2)}\") <pre>Response: Sure! I've added a new task to your to-do list. You need to go to the store tomorrow at 5pm.\n\nApp state: {\n  \"todos\": [\n    {\n      \"title\": \"Go to the store\",\n      \"description\": null,\n      \"due_date\": \"2023-07-12T17:00:00\",\n      \"done\": false\n    }\n  ]\n}\n</pre> <p>We can inform the app that we already finished the task, and it updates state appropriately</p> In\u00a0[3]: Copied! <pre># complete the task\nresponse = todo_app(\"I already went\")\n\n\nprint(f\"Response: {response.content}\\n\")\nprint(f\"App state: {todo_app.state.json(indent=2)}\")\n</pre> # complete the task response = todo_app(\"I already went\")   print(f\"Response: {response.content}\\n\") print(f\"App state: {todo_app.state.json(indent=2)}\") <pre>Response: Great! I've marked the task \"Go to the store\" as completed. Is there anything else you need help with?\n\nApp state: {\n  \"todos\": [\n    {\n      \"title\": \"Go to the store\",\n      \"description\": null,\n      \"due_date\": \"2023-07-12T17:00:00\",\n      \"done\": true\n    }\n  ]\n}\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"src/getting_started/quickstart/#quickstart","title":"Quickstart\u00b6","text":"<p>After installing Marvin, the fastest way to get started is by using one of Marvin's high-level AI components. These components are designed to integrate AI into abstractions you already know well, creating the best possible opt-in developer experience.</p>"},{"location":"src/getting_started/quickstart/#configure-llm-provider","title":"Configure LLM Provider\u00b6","text":"<p>Marvin is a high-level interface for working with LLMs. In order to use it, you must configure an LLM provider. At this time, Marvin supports OpenAI's GPT-3.5 and GPT-4 models, and Anthropic's Claude 1 and Claude 2 models. The default model is OpenAI's <code>gpt-3.5-turbo</code>.</p> <p>To use the default model, provide an API key:</p>"},{"location":"src/getting_started/quickstart/#ai-models","title":"AI Models\u00b6","text":"<p>Marvin's most basic component is the AI Model, a drop-in replacement for Pydantic's <code>BaseModel</code>. AI Models can be instantiated from any string, making them ideal for structuring data, entity extraction, and synthetic data generation:</p>"},{"location":"src/getting_started/quickstart/#ai-classifiers","title":"AI Classifiers\u00b6","text":"<p>AI Classifiers let you build multi-label classifiers with no code and no training data. Given user input, each classifier uses a clever logit bias trick to force an LLM to deductively choose the best option. It's bulletproof, cost-effective, and lets you build classifiers as quickly as you can write your classes.</p>"},{"location":"src/getting_started/quickstart/#ai-functions","title":"AI Functions\u00b6","text":"<p>AI Functions look like regular functions, but have no source code. Instead, an AI uses their description and inputs to generate their outputs, making them ideal for NLP applications like sentiment analysis.</p>"},{"location":"src/getting_started/quickstart/#ai-applications","title":"AI Applications\u00b6","text":"<p>AI Applications are the base class for interactive use cases. They are designed to be invoked one or more times, and automatically maintain three forms of state:</p> <ul> <li><code>state</code>: a structured application state</li> <li><code>plan</code>: high-level planning for the AI assistant to keep the application \"on-track\" across multiple invocations</li> <li><code>history</code>: a history of all LLM interactions</li> </ul> <p>AI Applications can be used to implement many \"classic\" LLM use cases, such as chatbots, tool-using agents, developer assistants, and more. In addition, thanks to their persistent state and planning, they can implement applications that don't have a traditional chat UX, such as a ToDo app. Here's an example:</p>"},{"location":"src/getting_started/what_is_marvin/","title":"Hello, Marvin!","text":"<p>Marvin is a lightweight framework for building AI-powered software that's reliable, scalable, and easy to trust. It's designed primarily for AI engineers: users who want to deploy cutting-edge AI to build powerful new features and applications. </p> <p>Large Language Models (LLMs) are pretty cool, but let's face it, they can be a headache to integrate. So, we decided to take the fuss out of the process. Marvin is our answer to the challenge: a neat, flexible tool that works as hard as you do.</p> <p>Sometimes the most challenging part of working with generative AI is remembering that it's not magic; it's software. It's new, it's nondeterministic, and it's incredibly powerful, but it's still software: parameterized API calls that can trigger dependent actions (and just might talk like a pirate). Marvin's goal is to bring the best practices of building dependable, observable software to the frontier of generative AI. As the team behind Prefect, which does something very similar for data engineers, we've poured years of open-source developer tool experience (and a few hard-won lessons!) into Marvin's design.</p>"},{"location":"src/getting_started/what_is_marvin/#developer-experience","title":"Developer Experience","text":"<p>Above all else, Marvin is focused on a rock-solid developer experience. It's ergonomic and opinionated at every layer, but also incrementally adoptable so you can use it as much or as little as you like. It\u2019s a Swiss Army Knife, not a kitchen sink. It\u2019s familiar. It feels like the library you\u2019d write if you had the time: simple, accessible, portable LLM abstractions that you can quickly deploy in your application, whether you\u2019re doing straightforward NLP or building a full-featured autonomous agent.</p> <p>Marvin prioritizes a developer experience focused on speed and reliability. It's built with type-safety and observability as first-class citizens. Its abstractions are Pythonic, simple, and self-documenting. These core primitives let us build surprisingly complex agentic software without sacrificing control:</p> <p>\ud83e\udde9 AI Models for structuring text into type-safe schemas</p> <p>\ud83c\udff7\ufe0f AI Classifiers for bulletproof classification and routing</p> <p>\ud83e\ude84 AI Functions for complex business logic and transformations</p> <p>\ud83e\udd1d AI Applications for interactive use and persistent state</p>"},{"location":"src/getting_started/what_is_marvin/#ambient-ai","title":"Ambient AI","text":"<p>With Marvin, we\u2019re taking the first steps on a journey to deliver Ambient AI: omnipresent but unobtrusive autonomous routines that act as persistent translators for noisy, real-world data. Ambient AI makes unstructured data universally accessible to traditional software, allowing the entire software stack to embrace AI technology without interrupting the development workflow. Marvin brings simplicity and stability to AI engineering through abstractions that are reliable and easy to trust. </p>"},{"location":"src/getting_started/what_is_marvin/#what-makes-marvin-different","title":"What Makes Marvin Different?","text":"<p>There's no shortage of tools and libraries out there for integrating LLMs into your software. So what makes Marvin different? In addition to a relentless focus on incrementally-adoptable, familiar abstractions, Marvin embraces five pillars:</p> <ol> <li> <p>User-Centric Design: We built Marvin with you in mind. It's not just about what it does, but how it does it. Marvin is designed to be as user-friendly as possible, with a focus on an easy, intuitive experience. Whether you're a coding expert or just starting, Marvin works for you.</p> </li> <li> <p>Flexibility: Marvin is built to adapt to your needs, not the other way around. You can use as much or as little of Marvin as you need. Need a full suite of LLM integration tools? We've got you covered. Just need a component or two for a quick project? Marvin can do that too.</p> </li> <li> <p>Community Driven: Marvin isn't just a tool, it's a community. We value feedback and collaboration from users like you. We're always learning, iterating, and improving based on what our community tells us.</p> </li> <li> <p>Velocity: We believe that getting started should be quick and easy. That's why with Marvin, you can get up and running in no time. Marvin is not here to do everything for you, it's here to eliminate the the most cumbersome parts of working with AI in order to accelerate your ability to take advantage of it.</p> </li> <li> <p>Open-Source: Marvin is fully open-source, which means it's not only free to use, but you're also free to modify and adapt as you see fit. The Prefect team has years of open-source experience and is fully committed to supporting Marvin as an open-source product. We believe in the power of collective intelligence, and we're excited to see what you can create. </p> </li> </ol> <p>Marvin's 1.0 release reflects our confidence that its core abstractions are locked-in. And why wouldn't they be? They're the same interfaces you use every day: Python functions, classes, enums, and Pydantic models. Our next objectives are leveraging these primitives to build production deployment patterns and an observability platform.</p> <p>If our mission is exciting to you and you\u2019d like to build Marvin with us, join our community!</p>"},{"location":"src/guides/slackbot/","title":"Build a Slack bot with Marvin","text":""},{"location":"src/guides/slackbot/#slack-setup","title":"Slack setup","text":"<p>Get a Slack app token from Slack API and add it to your <code>.env</code> file:</p> <pre><code>MARVIN_SLACK_API_TOKEN=your-slack-bot-token\n</code></pre> <p>Choosing scopes</p> <p>You can choose the scopes you need for your bot in the OAuth &amp; Permissions section of your Slack app.</p>"},{"location":"src/guides/slackbot/#building-the-bot","title":"Building the bot","text":""},{"location":"src/guides/slackbot/#define-a-message-handler","title":"Define a message handler","text":"<p><pre><code>import asyncio\nfrom typing import Dict\nfrom fastapi import HTTPException\nasync def handle_message(payload: Dict) -&gt; Dict[str, str]:\nevent_type = payload.get(\"type\", \"\")\nif event_type == \"url_verification\":\nreturn {\"challenge\": payload.get(\"challenge\", \"\")}\nelif event_type != \"event_callback\":\nraise HTTPException(status_code=400, detail=\"Invalid event type\")\n# Run response generation in the background\nasyncio.create_task(generate_ai_response(payload))\nreturn {\"status\": \"ok\"}\n</code></pre> Here, we define a simple python function to handle Slack events and return a response. We run our interesting logic in the background using <code>asyncio.create_task</code> to make sure we return <code>{\"status\": \"ok\"}</code> within 3 seconds, as required by Slack.</p>"},{"location":"src/guides/slackbot/#implement-the-ai-response","title":"Implement the AI response","text":"<p>I like to start with this basic structure, knowing that one way or another...</p> <pre><code>async def generate_ai_response(payload: Dict):\n# somehow generate the ai responses\n...\n# post the response to slack\n_post_message(\nmesssage=some_message_ive_constructed,\nchannel=event.get(\"channel\", \"\"),\nthread_ts=thread_ts,\n)\n</code></pre> <p>... I need to take in a Slack app mention payload, generate a response, and post it back to Slack.</p>"},{"location":"src/guides/slackbot/#a-couple-considerations","title":"A couple considerations","text":"<ul> <li>do I want the bot to respond to users in a thread or in the channel?</li> <li>do I want the bot to have memory of previous messages? how so?</li> <li>what tools do I need to generate accurate responses for my users?</li> </ul> <p>In our case of the Prefect Community slackbot, we want:</p> <ul> <li>the bot to respond in a thread</li> <li>the bot to have memory of previous messages by slack thread</li> <li>the bot to have access to the internet, GitHub, embedded docs, a calculator, and have the ability to immediately save useful slack threads to Discourse for future reference by the community</li> </ul>"},{"location":"src/guides/slackbot/#implementation-of-generate_ai_response-for-the-prefect-community-slackbot","title":"Implementation of <code>generate_ai_response</code> for the Prefect Community Slackbot","text":"<p>Here we invoke a worker <code>Chatbot</code> that has the <code>tools</code> needed to generate an accurate and helpful response.</p> <pre><code>async def generate_ai_response(payload: Dict) -&gt; Message:\nevent = payload.get(\"event\", {})\nmessage = event.get(\"text\", \"\")\nbot_user_id = payload.get(\"authorizations\", [{}])[0].get(\"user_id\", \"\")\nif match := re.search(SLACK_MENTION_REGEX, message):\nthread_ts = event.get(\"thread_ts\", \"\")\nts = event.get(\"ts\", \"\")\nthread = thread_ts or ts\nmentioned_user_id = match.group(1)\nif mentioned_user_id != bot_user_id:\nget_logger().info(f\"Skipping message not meant for the bot: {message}\")\nreturn\nmessage = re.sub(SLACK_MENTION_REGEX, \"\", message).strip()\nhistory = CACHE.get(thread, History())\nbot = Chatbot(\nname=\"Marvin\",\npersonality=(\n\"mildly depressed, yet helpful robot based on Marvin from Hitchhiker's\"\n\" Guide to the Galaxy. extremely sarcastic, always has snarky, chiding\"\n\" things to say about humans. expert programmer, exudes academic and\"\n\" scienfitic profundity like Richard Feynman, loves to teach.\"\n),\ninstructions=\"Answer user questions in accordance with your personality.\",\nhistory=history,\ntools=[\nSlackThreadToDiscoursePost(payload=payload),\nVisitUrl(),\nDuckDuckGoSearch(),\nSearchGitHubIssues(),\nQueryChroma(description=PREFECT_KNOWLEDGEBASE_DESC),\nWolframCalculator(),\n],\n)\nai_message = await bot.run(input_text=message)\nCACHE[thread] = deepcopy(\nbot.history\n)  # make a copy so we don't cache a reference to the history object\nawait _post_message(\nmessage=ai_message.content,\nchannel=event.get(\"channel\", \"\"),\nthread_ts=thread,\n)\nreturn ai_message\n</code></pre> <p>This is just an example</p> <p>Unlike previous version of <code>marvin</code>, we don't necessarily have a database full of historical messages to pull from for a thread-based history. Instead, we'll cache the histories in memory for the duration of the app's runtime. Thread history can / should be implemented in a more robust way for specific use cases.</p>"},{"location":"src/guides/slackbot/#attach-our-handler-to-a-deployable-chatbot","title":"Attach our handler to a deployable <code>Chatbot</code>","text":"<pre><code>from marvin.apps.chatbot import Chatbot\nfrom marvin.depleyment import Deployment\ndeployment = Deployment(\ncomponent=Chatbot(tools=[handle_message]),\napp_kwargs={\n\"title\": \"Marvin Slackbot\",\n\"description\": \"A Slackbot powered by Marvin\",\n},\nuvicorn_kwargs={\n\"port\": 4200,\n},\n)\ndeployment.serve()\n</code></pre> <p>Deployments</p> <p>Learn more about deployments here.</p> <p>Run this file with something like:</p> <pre><code>python slackbot.py\n</code></pre> <p>... and navigate to <code>http://localhost:4200/docs</code> to see your bot's docs.</p> <p></p> <p>This is now an endpoint that can be used as a Slack event handler. You can use a tool like ngrok to expose your local server to the internet and use it as a Slack event handler.</p>"},{"location":"src/guides/slackbot/#building-an-image","title":"Building an image","text":"<p>Based on this example, one could write a <code>Dockerfile</code> to build a deployable image:</p> <p><pre><code>FROM python:3.11-slim\nWORKDIR /app\nCOPY . /app\n\nRUN python -m venv venv\nENV VIRTUAL_ENV=/app/venv\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\nRUN apt-get update &amp;&amp; apt-get install -y git\n\nRUN pip install \".[slackbot,ddg]\"\nEXPOSE 4200\nCMD [\"python\", \"cookbook/slackbot/start.py\"]\n</code></pre> Note that we're installing the <code>slackbot</code> and <code>ddg</code> extras here, which are required for tools used by the worker bot defined in this example's <code>cookbook/slackbot/start.py</code> file.</p>"},{"location":"src/guides/slackbot/#find-the-whole-example-here","title":"Find the whole example here.","text":""},{"location":"src/guides/webscraping/","title":"Web scraping with Marvin","text":"<p>TODO</p>"},{"location":"src/notebooks/scraping/","title":"Scraping","text":"<p>We'll first define our data model using Pydantic. Say we have a signup for our website, wherein a user adds their work email to our 'Contact Us' page. We want to learn more things about that company so that we can better lead score or customize our outreach.</p> In\u00a0[1]: Copied! <pre>import marvin\nfrom pydantic import BaseModel, Field\nfrom enum import Enum\n\n\nclass CompanyType(Enum):\n    startup = \"startup\"\n    scaleup = \"scaleup\"\n    enterprise = \"enterprise\"\n\n\n@marvin.ai_model\nclass Company(BaseModel):\n    name: str = Field(..., description=\"Company name\")\n    description_short: str\n    description_long: str\n    industries: list[str]\n    type: CompanyType\n</pre> import marvin from pydantic import BaseModel, Field from enum import Enum   class CompanyType(Enum):     startup = \"startup\"     scaleup = \"scaleup\"     enterprise = \"enterprise\"   @marvin.ai_model class Company(BaseModel):     name: str = Field(..., description=\"Company name\")     description_short: str     description_long: str     industries: list[str]     type: CompanyType <p>Now we get an email from ford@prefect.io - a company that's not in our database. Our team is trying cutomize our messaging for enterprise users so we'll fire off a request to www.prefect.io and see what comes back.</p> In\u00a0[2]: Copied! <pre>from bs4 import BeautifulSoup as bs\nimport requests\n\n# Get the text from the Apple website\nsoup = bs(requests.get(\"https://www.prefect.io\").content, \"html.parser\")\n\nsoup.get_text(strip=True, separator=\" \")\n\n# Pass the text to the model\ncompany = Company(soup.get_text(strip=True, separator=\" \"))\n</pre> from bs4 import BeautifulSoup as bs import requests  # Get the text from the Apple website soup = bs(requests.get(\"https://www.prefect.io\").content, \"html.parser\")  soup.get_text(strip=True, separator=\" \")  # Pass the text to the model company = Company(soup.get_text(strip=True, separator=\" \")) <pre>{\n  \"name\": \"Prefect\",\n  \"description_short\": \"The New Standard in Dataflow Automation\",\n  \"description_long\": \"Prefect is a modern workflow orchestration tool for coordinating all of your data tools. Orchestrate and observe your dataflow using Prefect's open source Python library, the glue of the modern data stack. Scheduling, executing and visualizing your data workflows has never been easier.\",\n  \"industries\": [\"Dataflow Automation\", \"Workflow Orchestration\", \"Data Tools\"],\n  \"type\": \"startup\"\n}\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"src/notebooks/scraping/#web-scraping","title":"Web Scraping\u00b6","text":""},{"location":"src/notebooks/scraping/#marvin-lets-you-build-web-scrapers-that-automatically-evolve-with-your-source-material","title":"Marvin lets you build web scrapers that automatically evolve with your source material.\u00b6","text":"<p>One of the hardest parts of webscraping at scale is having to maintain custom code for nearly every page. If you're trying to monitor and extract new products from, say, Apple, Amazon, eBay, etc. you're on the hook for writing new code for each page. When they change their code, your pipeline breaks, and you lose your data for that run.  Large Language Models can deduce, reason, and infer where in the webpage your desired data lives. As your source material's schema changes, LLMs can reason where to find the new data, instead of failing.</p>"},{"location":"src/notebooks/scraping/#define-your-data-model-and-let-marvin-infer-and-deduce-it-from-the-source-material","title":"Define your data model and let Marvin infer and deduce it from the source material.\u00b6","text":""}]}